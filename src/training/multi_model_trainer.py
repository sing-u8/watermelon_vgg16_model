"""
Multi-Model Trainer Module
ë‹¤ì¤‘ ëª¨ë¸ ì•„í‚¤í…ì²˜ ì§€ì› í†µí•© íŠ¸ë ˆì´ë„ˆ
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR, CosineAnnealingLR
from typing import Dict, Any, Optional, List, Tuple, Union
import yaml
from pathlib import Path
import time
import numpy as np
from tqdm import tqdm
import warnings

# ìƒëŒ€ import
import sys
from pathlib import Path
current_dir = Path(__file__).parent
src_dir = current_dir.parent
sys.path.insert(0, str(src_dir))

from models.vgg_watermelon import VGGWatermelon, save_model_checkpoint
from models.efficientnet_watermelon import EfficientNetWatermelon, save_efficientnet_checkpoint
from models.melspec_cnn_watermelon import MelSpecCNNWatermelon, save_melspec_cnn_checkpoint
from training.data_loader import WatermelonDataLoader
from training.loss_functions import create_loss_function
from training.metrics import RegressionMetrics, MetricsTracker

# ì§€ì› ëª¨ë¸ íƒ€ì…
ModelType = Union[VGGWatermelon, EfficientNetWatermelon, MelSpecCNNWatermelon]


class MultiModelTrainer:
    """
    ë‹¤ì¤‘ ëª¨ë¸ ì•„í‚¤í…ì²˜ ì§€ì› í†µí•© íŠ¸ë ˆì´ë„ˆ í´ë˜ìŠ¤
    """
    
    def __init__(self, 
                 model: ModelType,
                 data_loader: WatermelonDataLoader,
                 loss_function: nn.Module,
                 optimizer: optim.Optimizer,
                 device: torch.device,
                 scheduler: Optional[optim.lr_scheduler._LRScheduler] = None,
                 save_dir: str = "experiments",
                 experiment_name: str = "multi_model_training",
                 model_type: str = "vgg"):
        """
        MultiModelTrainer ì´ˆê¸°í™”
        
        Args:
            model (ModelType): í›ˆë ¨í•  ëª¨ë¸
            data_loader (WatermelonDataLoader): ë°ì´í„° ë¡œë”
            loss_function (nn.Module): ì†ì‹¤ í•¨ìˆ˜
            optimizer (optim.Optimizer): ì˜µí‹°ë§ˆì´ì €
            device (torch.device): í•™ìŠµ ë””ë°”ì´ìŠ¤
            scheduler (optim.lr_scheduler._LRScheduler, optional): í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
            save_dir (str): ì €ì¥ ë””ë ‰í† ë¦¬
            experiment_name (str): ì‹¤í—˜ ì´ë¦„
            model_type (str): ëª¨ë¸ íƒ€ì… ("vgg", "efficientnet", "melspec_cnn")
        """
        self.model = model.to(device)
        self.data_loader = data_loader
        self.loss_function = loss_function
        self.optimizer = optimizer
        self.device = device
        self.scheduler = scheduler
        self.save_dir = Path(save_dir)
        self.experiment_name = experiment_name
        self.model_type = model_type
        
        # ì‹¤í—˜ ë””ë ‰í† ë¦¬ ìƒì„±
        self.experiment_dir = self.save_dir / experiment_name
        self.experiment_dir.mkdir(parents=True, exist_ok=True)
        
        # ë©”íŠ¸ë¦­ ì¶”ì ê¸° ì´ˆê¸°í™”
        self.metrics_tracker = MetricsTracker()
        
        # ìµœì  ëª¨ë¸ ìƒíƒœ ì¶”ì 
        self.best_val_loss = float('inf')
        self.best_val_mae = float('inf')
        self.best_epoch = 0
        
        # ì‹œê°„ ì¸¡ì •
        self.start_time = None
        self.total_training_time = 0
        
        print(f"ğŸš€ MultiModelTrainer ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   ğŸ“Š ëª¨ë¸ íƒ€ì…: {model_type}")
        print(f"   ğŸ¯ ì‹¤í—˜ ì´ë¦„: {experiment_name}")
        print(f"   ğŸ“ ì €ì¥ ê²½ë¡œ: {self.experiment_dir}")
        print(f"   ğŸ”§ ë””ë°”ì´ìŠ¤: {device}")
        
    def train_epoch(self) -> Dict[str, float]:
        """í•œ ì—í¬í¬ í›ˆë ¨"""
        self.model.train()
        train_loader = self.data_loader.get_train_loader()
        
        total_loss = 0.0
        predictions = []
        targets = []
        
        # ì§„í–‰ ë°” ì„¤ì •
        pbar = tqdm(train_loader, desc="Training", leave=False)
        
        for batch_idx, (data, target) in enumerate(pbar):
            data, target = data.to(self.device), target.to(self.device)
            
            # ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”
            self.optimizer.zero_grad()
            
            # ìˆœì „íŒŒ
            output = self.model(data)
            loss = self.loss_function(output, target)
            
            # ì—­ì „íŒŒ
            loss.backward()
            self.optimizer.step()
            
            # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            total_loss += loss.item()
            predictions.extend(output.detach().cpu().numpy())
            targets.extend(target.detach().cpu().numpy())
            
            # ì§„í–‰ ë°” ì—…ë°ì´íŠ¸
            pbar.set_postfix({'loss': loss.item():.4f})
        
        # ì—í¬í¬ ë©”íŠ¸ë¦­ ê³„ì‚°
        avg_loss = total_loss / len(train_loader)
        predictions = np.array(predictions).flatten()
        targets = np.array(targets).flatten()
        
        metrics = RegressionMetrics.calculate_metrics(predictions, targets)
        metrics['loss'] = avg_loss
        
        return metrics
    
    def validate_epoch(self) -> Dict[str, float]:
        """í•œ ì—í¬í¬ ê²€ì¦"""
        self.model.eval()
        val_loader = self.data_loader.get_val_loader()
        
        total_loss = 0.0
        predictions = []
        targets = []
        
        with torch.no_grad():
            pbar = tqdm(val_loader, desc="Validating", leave=False)
            
            for data, target in pbar:
                data, target = data.to(self.device), target.to(self.device)
                
                # ìˆœì „íŒŒ
                output = self.model(data)
                loss = self.loss_function(output, target)
                
                # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                total_loss += loss.item()
                predictions.extend(output.cpu().numpy())
                targets.extend(target.cpu().numpy())
                
                # ì§„í–‰ ë°” ì—…ë°ì´íŠ¸
                pbar.set_postfix({'loss': loss.item():.4f})
        
        # ì—í¬í¬ ë©”íŠ¸ë¦­ ê³„ì‚°
        avg_loss = total_loss / len(val_loader)
        predictions = np.array(predictions).flatten()
        targets = np.array(targets).flatten()
        
        metrics = RegressionMetrics.calculate_metrics(predictions, targets)
        metrics['loss'] = avg_loss
        
        return metrics
    
    def test_model(self) -> Dict[str, float]:
        """ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
        self.model.eval()
        test_loader = self.data_loader.get_test_loader()
        
        total_loss = 0.0
        predictions = []
        targets = []
        
        with torch.no_grad():
            pbar = tqdm(test_loader, desc="Testing", leave=False)
            
            for data, target in pbar:
                data, target = data.to(self.device), target.to(self.device)
                
                # ìˆœì „íŒŒ
                output = self.model(data)
                loss = self.loss_function(output, target)
                
                # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
                total_loss += loss.item()
                predictions.extend(output.cpu().numpy())
                targets.extend(target.cpu().numpy())
                
                # ì§„í–‰ ë°” ì—…ë°ì´íŠ¸
                pbar.set_postfix({'loss': loss.item():.4f})
        
        # í…ŒìŠ¤íŠ¸ ë©”íŠ¸ë¦­ ê³„ì‚°
        avg_loss = total_loss / len(test_loader)
        predictions = np.array(predictions).flatten()
        targets = np.array(targets).flatten()
        
        metrics = RegressionMetrics.calculate_metrics(predictions, targets)
        metrics['loss'] = avg_loss
        
        return metrics
    
    def save_checkpoint(self, 
                       metrics: Dict[str, Any], 
                       epoch: int,
                       is_best: bool = False,
                       checkpoint_type: str = "epoch"):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ëª¨ë¸ íƒ€ì…ë³„)"""
        checkpoint_name = f"{checkpoint_type}_checkpoint"
        if is_best:
            checkpoint_name = "best_model"
        
        save_path = self.experiment_dir / f"{checkpoint_name}.pth"
        
        # ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ ì €ì¥ í•¨ìˆ˜ ì‚¬ìš©
        if self.model_type == "vgg":
            save_model_checkpoint(
                self.model,
                str(save_path),
                epoch=epoch,
                loss=metrics.get('loss', 0.0),
                optimizer_state=self.optimizer.state_dict(),
                scheduler_state=self.scheduler.state_dict() if self.scheduler else None,
                metrics=metrics
            )
        elif self.model_type == "efficientnet":
            save_efficientnet_checkpoint(
                self.model,
                str(save_path),
                epoch=epoch,
                loss=metrics.get('loss', 0.0),
                optimizer_state=self.optimizer.state_dict(),
                scheduler_state=self.scheduler.state_dict() if self.scheduler else None,
                metrics=metrics
            )
        elif self.model_type == "melspec_cnn":
            save_melspec_cnn_checkpoint(
                self.model,
                str(save_path),
                epoch=epoch,
                loss=metrics.get('loss', 0.0),
                optimizer_state=self.optimizer.state_dict(),
                scheduler_state=self.scheduler.state_dict() if self.scheduler else None,
                metrics=metrics
            )
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {self.model_type}")
    
    def train(self, 
             num_epochs: int,
             save_every: int = 5,
             validate_every: int = 1,
             early_stopping: bool = True,
             patience: int = 5,
             verbose: bool = True) -> Dict[str, Any]:
        """ëª¨ë¸ í›ˆë ¨"""
        
        print(f"ğŸš€ {self.model_type.upper()} ëª¨ë¸ í›ˆë ¨ ì‹œì‘!")
        print(f"   ğŸ“Š ì—í¬í¬: {num_epochs}")
        print(f"   ğŸ¯ Early Stopping: {early_stopping} (patience: {patience})")
        print(f"   ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì£¼ê¸°: {save_every}")
        
        self.start_time = time.time()
        
        # Early stopping ë³€ìˆ˜
        patience_counter = 0
        
        for epoch in range(1, num_epochs + 1):
            epoch_start_time = time.time()
            
            # í›ˆë ¨
            train_metrics = self.train_epoch()
            
            # ê²€ì¦
            val_metrics = None
            if epoch % validate_every == 0:
                val_metrics = self.validate_epoch()
                
                # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
                if self.scheduler:
                    if isinstance(self.scheduler, ReduceLROnPlateau):
                        self.scheduler.step(val_metrics['loss'])
                    else:
                        self.scheduler.step()
            
            # ë©”íŠ¸ë¦­ ì¶”ì 
            self.metrics_tracker.add_epoch_metrics(epoch, train_metrics, val_metrics)
            
            # ìµœì  ëª¨ë¸ ì²´í¬
            is_best = False
            if val_metrics and val_metrics['loss'] < self.best_val_loss:
                self.best_val_loss = val_metrics['loss']
                self.best_val_mae = val_metrics['mae']
                self.best_epoch = epoch
                is_best = True
                patience_counter = 0
            else:
                patience_counter += 1
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            if epoch % save_every == 0 or is_best:
                current_metrics = val_metrics if val_metrics else train_metrics
                self.save_checkpoint(current_metrics, epoch, is_best)
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            if verbose:
                epoch_time = time.time() - epoch_start_time
                print(f"ğŸ“Š Epoch {epoch:3d}/{num_epochs}")
                print(f"   ğŸƒ Train Loss: {train_metrics['loss']:.4f}, MAE: {train_metrics['mae']:.4f}")
                if val_metrics:
                    print(f"   âœ… Val Loss: {val_metrics['loss']:.4f}, MAE: {val_metrics['mae']:.4f}")
                print(f"   â±ï¸ Time: {epoch_time:.2f}s")
                if self.scheduler:
                    current_lr = self.scheduler.get_last_lr()[0]
                    print(f"   ğŸ“ˆ LR: {current_lr:.2e}")
                print()
            
            # Early stopping ì²´í¬
            if early_stopping and patience_counter >= patience:
                print(f"ğŸ›‘ Early stopping at epoch {epoch} (patience: {patience})")
                break
        
        self.total_training_time = time.time() - self.start_time
        
        # ìµœì¢… í…ŒìŠ¤íŠ¸
        print("ğŸ§ª ìµœì¢… í…ŒìŠ¤íŠ¸ ì‹¤í–‰...")
        test_metrics = self.test_model()
        
        # í›ˆë ¨ ê²°ê³¼ ì €ì¥
        training_results = {
            'best_epoch': self.best_epoch,
            'best_val_loss': self.best_val_loss,
            'best_val_mae': self.best_val_mae,
            'test_metrics': test_metrics,
            'total_training_time': self.total_training_time,
            'model_type': self.model_type,
            'experiment_name': self.experiment_name
        }
        
        self._save_training_results(training_results)
        
        # ì„±ê³µ íŒŒì¼ ìƒì„±
        success_file = self.experiment_dir / "SUCCESS"
        success_file.write_text(f"Training completed successfully!\n"
                               f"Best epoch: {self.best_epoch}\n"
                               f"Best val MAE: {self.best_val_mae:.4f}\n"
                               f"Test MAE: {test_metrics['mae']:.4f}\n"
                               f"Model type: {self.model_type}\n"
                               f"Total time: {self.total_training_time:.2f}s")
        
        print(f"ğŸ‰ {self.model_type.upper()} ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!")
        print(f"   ğŸ† ìµœê³  ì„±ëŠ¥ (Epoch {self.best_epoch}): Val MAE {self.best_val_mae:.4f}")
        print(f"   ğŸ§ª í…ŒìŠ¤íŠ¸ ì„±ëŠ¥: MAE {test_metrics['mae']:.4f}")
        print(f"   â±ï¸ ì´ í›ˆë ¨ ì‹œê°„: {self.total_training_time:.2f}ì´ˆ")
        
        return training_results
    
    def _save_training_results(self, results: Dict[str, Any]):
        """í›ˆë ¨ ê²°ê³¼ ì €ì¥"""
        results_file = self.experiment_dir / "training_results.yaml"
        with open(results_file, 'w') as f:
            yaml.dump(results, f, default_flow_style=False)
        
        print(f"ğŸ’¾ í›ˆë ¨ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {results_file}")


def create_model_from_config(config: Dict[str, Any], model_type: str) -> ModelType:
    """ì„¤ì •ì—ì„œ ëª¨ë¸ ìƒì„±"""
    model_config = config.get('model', {})
    
    if model_type == "vgg":
        from models.vgg_watermelon import create_vgg_watermelon
        return create_vgg_watermelon(**model_config)
    elif model_type == "efficientnet":
        from models.efficientnet_watermelon import create_efficientnet_watermelon
        return create_efficientnet_watermelon(**model_config)
    elif model_type == "melspec_cnn":
        from models.melspec_cnn_watermelon import create_melspec_cnn_watermelon
        return create_melspec_cnn_watermelon(**model_config)
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {model_type}")


def create_multi_model_trainer_from_config(config_path: str, 
                                         data_path: str,
                                         model_type: str = "vgg",
                                         experiment_name: Optional[str] = None) -> MultiModelTrainer:
    """
    ì„¤ì • íŒŒì¼ì—ì„œ MultiModelTrainer ìƒì„±
    
    Args:
        config_path (str): ì„¤ì • íŒŒì¼ ê²½ë¡œ
        data_path (str): ë°ì´í„° ê²½ë¡œ
        model_type (str): ëª¨ë¸ íƒ€ì… ("vgg", "efficientnet", "melspec_cnn")
        experiment_name (str, optional): ì‹¤í—˜ ì´ë¦„
        
    Returns:
        MultiModelTrainer: ìƒì„±ëœ íŠ¸ë ˆì´ë„ˆ
    """
    # ì„¤ì • ë¡œë“œ
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # ì‹¤í—˜ ì´ë¦„ ì„¤ì •
    if experiment_name is None:
        experiment_name = config.get('experiment_name', f'{model_type}_experiment')
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device_config = config.get('device', 'auto')
    if device_config == 'auto':
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    else:
        device = torch.device(device_config)
    
    # ëª¨ë¸ ìƒì„±
    model = create_model_from_config(config, model_type)
    
    # ë°ì´í„° ë¡œë” ìƒì„±
    data_loader = WatermelonDataLoader(
        data_root=data_path,
        **config.get('data', {})
    )
    
    # ì†ì‹¤ í•¨ìˆ˜ ìƒì„±
    loss_config = config.get('training', {})
    loss_function = create_loss_function(
        loss_config.get('loss_function', 'mse'),
        huber_delta=loss_config.get('huber_delta', 1.0)
    )
    
    # ì˜µí‹°ë§ˆì´ì € ìƒì„±
    optimizer_name = loss_config.get('optimizer', 'adam')
    learning_rate = loss_config.get('learning_rate', 0.001)
    weight_decay = loss_config.get('weight_decay', 1e-4)
    
    if optimizer_name.lower() == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    elif optimizer_name.lower() == 'sgd':
        optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay, momentum=0.9)
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì˜µí‹°ë§ˆì´ì €: {optimizer_name}")
    
    # ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„±
    scheduler = None
    scheduler_config = loss_config.get('scheduler', 'step')
    if scheduler_config == 'step':
        step_size = loss_config.get('step_size', 7)
        gamma = loss_config.get('gamma', 0.1)
        scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)
    elif scheduler_config == 'plateau':
        scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)
    
    # íŠ¸ë ˆì´ë„ˆ ìƒì„±
    trainer = MultiModelTrainer(
        model=model,
        data_loader=data_loader,
        loss_function=loss_function,
        optimizer=optimizer,
        device=device,
        scheduler=scheduler,
        save_dir=config.get('logging', {}).get('save_dir', 'experiments'),
        experiment_name=experiment_name,
        model_type=model_type
    )
    
    return trainer 