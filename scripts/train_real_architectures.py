#!/usr/bin/env python3
"""
Real Architecture Training Script
ì‹¤ì œ EfficientNetê³¼ MelSpecCNN ì•„í‚¤í…ì²˜ë¡œ í›ˆë ¨í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""

import os
import sys
import argparse
import yaml
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from pathlib import Path
from datetime import datetime
import json
import matplotlib.pyplot as plt
from tqdm import tqdm
import warnings

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# ëª¨ë¸ ë° ìœ í‹¸ë¦¬í‹° import
from src.models.efficientnet_watermelon import create_efficientnet_watermelon
from src.models.melspec_cnn_watermelon import create_melspec_cnn_watermelon
from src.data.watermelon_dataset import WatermelonDataset
from src.training.data_loader import create_data_loaders_stratified
from src.training.loss_functions import get_loss_function
from src.training.metrics import calculate_metrics

warnings.filterwarnings('ignore')


class RealArchitectureTrainer:
    """ì‹¤ì œ ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•˜ëŠ” í›ˆë ¨ í´ë˜ìŠ¤"""
    
    def __init__(self, model_type: str, config: dict, save_dir: str):
        self.model_type = model_type
        self.config = config
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ–¥ï¸ ë””ë°”ì´ìŠ¤: {self.device}")
        
        # ëª¨ë¸ ìƒì„±
        self.model = self._create_model()
        self.model.to(self.device)
        
        # ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € ì„¤ì •
        self.criterion = get_loss_function(
            config['training']['loss_function'],
            config['training'].get('huber_delta', 1.0)
        )
        
        self.optimizer = self._create_optimizer()
        self.scheduler = self._create_scheduler()
        
        # í›ˆë ¨ ê¸°ë¡
        self.train_losses = []
        self.val_losses = []
        self.train_maes = []
        self.val_maes = []
        self.best_val_mae = float('inf')
        self.best_epoch = 0
        
    def _create_model(self) -> nn.Module:
        """ëª¨ë¸ ìƒì„±"""
        model_config = self.config['model']
        
        if self.model_type == 'efficientnet':
            model = create_efficientnet_watermelon(
                pretrained=model_config.get('pretrained', False),
                dropout_rate=model_config.get('dropout_rate', 0.7),
                num_fc_layers=model_config.get('num_fc_layers', 2),
                fc_hidden_size=model_config.get('fc_hidden_size', 256)
            )
            print("âœ… EfficientNet ëª¨ë¸ ìƒì„± ì™„ë£Œ")
            
        elif self.model_type == 'melspec_cnn':
            model = create_melspec_cnn_watermelon(
                input_channels=model_config.get('input_channels', 3),
                base_channels=model_config.get('base_channels', 32),
                dropout_rate=model_config.get('dropout_rate', 0.7)
            )
            print("âœ… MelSpecCNN ëª¨ë¸ ìƒì„± ì™„ë£Œ")
            
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {self.model_type}")
        
        # ëª¨ë¸ ì •ë³´ ì¶œë ¥
        model.print_model_info()
        return model
    
    def _create_optimizer(self) -> optim.Optimizer:
        """ì˜µí‹°ë§ˆì´ì € ìƒì„±"""
        optimizer_name = self.config['training'].get('optimizer', 'adam').lower()
        lr = self.config['training']['learning_rate']
        weight_decay = self.config['training'].get('weight_decay', 1e-4)
        
        if optimizer_name == 'adam':
            return optim.Adam(self.model.parameters(), lr=lr, weight_decay=weight_decay)
        elif optimizer_name == 'sgd':
            momentum = self.config['training'].get('momentum', 0.9)
            return optim.SGD(self.model.parameters(), lr=lr, weight_decay=weight_decay, momentum=momentum)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì˜µí‹°ë§ˆì´ì €: {optimizer_name}")
    
    def _create_scheduler(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„±"""
        scheduler_type = self.config['training'].get('scheduler', 'step')
        
        if scheduler_type == 'step':
            step_size = self.config['training'].get('step_size', 7)
            gamma = self.config['training'].get('gamma', 0.7)
            return optim.lr_scheduler.StepLR(self.optimizer, step_size=step_size, gamma=gamma)
        elif scheduler_type == 'plateau':
            return optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', patience=3, factor=0.5)
        else:
            return None
    
    def train_epoch(self, train_loader: DataLoader) -> tuple:
        """í•œ ì—í¬í¬ í›ˆë ¨"""
        self.model.train()
        total_loss = 0.0
        total_mae = 0.0
        num_batches = 0
        
        progress_bar = tqdm(train_loader, desc="Training", leave=False)
        
        for batch_idx, (data, targets) in enumerate(progress_bar):
            data, targets = data.to(self.device), targets.to(self.device)
            targets = targets.float().unsqueeze(1)
            
            # Forward pass
            self.optimizer.zero_grad()
            outputs = self.model(data)
            loss = self.criterion(outputs, targets)
            
            # Backward pass
            loss.backward()
            self.optimizer.step()
            
            # ë©”íŠ¸ë¦­ ê³„ì‚°
            total_loss += loss.item()
            mae = torch.mean(torch.abs(outputs - targets)).item()
            total_mae += mae
            num_batches += 1
            
            # Progress bar ì—…ë°ì´íŠ¸
            progress_bar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'MAE': f'{mae:.4f}'
            })
        
        avg_loss = total_loss / num_batches
        avg_mae = total_mae / num_batches
        
        return avg_loss, avg_mae
    
    def validate_epoch(self, val_loader: DataLoader) -> tuple:
        """í•œ ì—í¬í¬ ê²€ì¦"""
        self.model.eval()
        total_loss = 0.0
        total_mae = 0.0
        num_batches = 0
        
        with torch.no_grad():
            for data, targets in val_loader:
                data, targets = data.to(self.device), targets.to(self.device)
                targets = targets.float().unsqueeze(1)
                
                outputs = self.model(data)
                loss = self.criterion(outputs, targets)
                
                total_loss += loss.item()
                mae = torch.mean(torch.abs(outputs - targets)).item()
                total_mae += mae
                num_batches += 1
        
        avg_loss = total_loss / num_batches
        avg_mae = total_mae / num_batches
        
        return avg_loss, avg_mae
    
    def train(self, train_loader: DataLoader, val_loader: DataLoader, num_epochs: int) -> dict:
        """ëª¨ë¸ í›ˆë ¨"""
        print(f"ğŸš€ {self.model_type.upper()} ëª¨ë¸ í›ˆë ¨ ì‹œì‘!")
        print(f"   ğŸ“Š ì—í¬í¬: {num_epochs}")
        print(f"   ğŸ–¥ï¸ ë””ë°”ì´ìŠ¤: {self.device}")
        print("=" * 60)
        
        early_stopping_patience = self.config['training']['early_stopping'].get('patience', 10)
        early_stopping_counter = 0
        
        for epoch in range(num_epochs):
            start_time = datetime.now()
            
            # í›ˆë ¨
            train_loss, train_mae = self.train_epoch(train_loader)
            val_loss, val_mae = self.validate_epoch(val_loader)
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
            if self.scheduler:
                if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):
                    self.scheduler.step(val_loss)
                else:
                    self.scheduler.step()
            
            # ê¸°ë¡ ì €ì¥
            self.train_losses.append(train_loss)
            self.val_losses.append(val_loss)
            self.train_maes.append(train_mae)
            self.val_maes.append(val_mae)
            
            # ìµœê³  ì„±ëŠ¥ ì—…ë°ì´íŠ¸
            if val_mae < self.best_val_mae:
                self.best_val_mae = val_mae
                self.best_epoch = epoch + 1
                early_stopping_counter = 0
                
                # ìµœê³  ëª¨ë¸ ì €ì¥
                self._save_checkpoint(epoch + 1, is_best=True)
                print("   ğŸ† BEST!")
            else:
                early_stopping_counter += 1
            
            # í›ˆë ¨ ì‹œê°„ ê³„ì‚°
            epoch_time = (datetime.now() - start_time).total_seconds()
            
            # í˜„ì¬ í•™ìŠµë¥ 
            current_lr = self.optimizer.param_groups[0]['lr']
            
            # ì—í¬í¬ ê²°ê³¼ ì¶œë ¥
            print(f"ğŸ“Š Epoch {epoch+1}/{num_epochs} ({epoch_time:.1f}s)")
            print(f"   ğŸš‚ Train - Loss: {train_loss:.3f}, MAE: {train_mae:.3f}")
            print(f"   ğŸ” Val   - Loss: {val_loss:.3f}, MAE: {val_mae:.3f}")
            print(f"   ğŸšï¸ LR: {current_lr:.2e}")
            
            # ì¡°ê¸° ì¢…ë£Œ í™•ì¸
            if early_stopping_counter >= early_stopping_patience:
                print(f"\nâ¹ï¸ ì¡°ê¸° ì¢…ë£Œ: {early_stopping_patience} ì—í¬í¬ ë™ì•ˆ ê°œì„  ì—†ìŒ")
                break
        
        # í›ˆë ¨ ì™„ë£Œ
        print("\nâœ… í›ˆë ¨ ì™„ë£Œ!")
        print(f"   ğŸ† ìµœê³  Val MAE: {self.best_val_mae:.4f} (ì—í¬í¬ {self.best_epoch})")
        
        # í›ˆë ¨ ê²°ê³¼ ì €ì¥
        self._save_training_results()
        self._plot_training_curves()
        
        return {
            'best_val_mae': self.best_val_mae,
            'best_epoch': self.best_epoch,
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'train_maes': self.train_maes,
            'val_maes': self.val_maes
        }
    
    def _save_checkpoint(self, epoch: int, is_best: bool = False):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'best_val_mae': self.best_val_mae,
            'model_type': self.model_type,
            'model_config': self.config['model']
        }
        
        if self.scheduler:
            checkpoint['scheduler_state_dict'] = self.scheduler.state_dict()
        
        if is_best:
            checkpoint_path = self.save_dir / "best_model.pth"
        else:
            checkpoint_path = self.save_dir / f"checkpoint_epoch_{epoch}.pth"
        
        torch.save(checkpoint, checkpoint_path)
    
    def _save_training_results(self):
        """í›ˆë ¨ ê²°ê³¼ ì €ì¥"""
        results = {
            'model_type': self.model_type,
            'best_val_mae': self.best_val_mae,
            'best_epoch': self.best_epoch,
            'total_epochs': len(self.train_losses),
            'final_train_mae': self.train_maes[-1],
            'final_val_mae': self.val_maes[-1],
            'config': self.config
        }
        
        with open(self.save_dir / "training_results.json", 'w') as f:
            json.dump(results, f, indent=2)
    
    def _plot_training_curves(self):
        """í›ˆë ¨ ê³¡ì„  ì‹œê°í™”"""
        plt.figure(figsize=(12, 4))
        
        # Loss ê³¡ì„ 
        plt.subplot(1, 2, 1)
        plt.plot(self.train_losses, label='Train Loss', color='blue')
        plt.plot(self.val_losses, label='Val Loss', color='red')
        plt.title(f'{self.model_type.upper()} - Training Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True)
        
        # MAE ê³¡ì„ 
        plt.subplot(1, 2, 2)
        plt.plot(self.train_maes, label='Train MAE', color='blue')
        plt.plot(self.val_maes, label='Val MAE', color='red')
        plt.axhline(y=self.best_val_mae, color='red', linestyle='--', 
                   label=f'Best Val MAE: {self.best_val_mae:.4f}')
        plt.title(f'{self.model_type.upper()} - Training MAE')
        plt.xlabel('Epoch')
        plt.ylabel('MAE')
        plt.legend()
        plt.grid(True)
        
        plt.tight_layout()
        plt.savefig(self.save_dir / "training_curves.png", dpi=300, bbox_inches='tight')
        plt.close()


def create_config(model_type: str) -> dict:
    """ëª¨ë¸ íƒ€ì…ë³„ ì„¤ì • ìƒì„±"""
    base_config = {
        'data': {
            'data_root': 'watermelon_sound_data',
            'train_ratio': 0.70,
            'val_ratio': 0.15,
            'test_ratio': 0.15,
            'batch_size': 16,
            'num_workers': 4,
            'pin_memory': True,
            'audio_params': {
                'sample_rate': 16000,
                'n_mels': 128,
                'n_fft': 2048,
                'hop_length': 512,
                'win_length': 2048,
                'f_min': 0.0,
                'f_max': 11025.0
            },
            'augmentation': {
                'enabled': True,
                'noise_factor': 0.005,
                'time_shift_max': 0.1,
                'pitch_shift_max': 2.0,
                'volume_change_max': 0.1
            },
            'normalization': {
                'mean': [0.485, 0.456, 0.406],
                'std': [0.229, 0.224, 0.225]
            }
        },
        'training': {
            'epochs': 30,
            'learning_rate': 0.0001,
            'weight_decay': 1e-4,
            'optimizer': 'adam',
            'scheduler': 'step',
            'step_size': 10,
            'gamma': 0.7,
            'loss_function': 'mse',
            'early_stopping': {
                'enabled': True,
                'patience': 10,
                'min_delta': 0.001
            }
        }
    }
    
    if model_type == 'efficientnet':
        base_config['model'] = {
            'pretrained': False,  # ì²˜ìŒë¶€í„° í›ˆë ¨
            'dropout_rate': 0.7,
            'num_fc_layers': 2,
            'fc_hidden_size': 256
        }
    elif model_type == 'melspec_cnn':
        base_config['model'] = {
            'input_channels': 3,
            'base_channels': 32,
            'dropout_rate': 0.7
        }
    
    return base_config


def train_model(model_type: str, data_root: str) -> dict:
    """ë‹¨ì¼ ëª¨ë¸ í›ˆë ¨"""
    # ì„¤ì • ìƒì„±
    config = create_config(model_type)
    
    # ì €ì¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    save_dir = f"experiments/real_{model_type}_{timestamp}"
    
    # ë°ì´í„° ë¡œë” ìƒì„±
    print(f"ğŸ“‚ ë°ì´í„° ë¡œë” ìƒì„± ì¤‘... ({model_type})")
    train_loader, val_loader, test_loader = create_data_loaders_stratified(
        data_root=data_root,
        train_ratio=config['data']['train_ratio'],
        val_ratio=config['data']['val_ratio'],
        test_ratio=config['data']['test_ratio'],
        batch_size=config['data']['batch_size'],
        num_workers=config['data']['num_workers'],
        pin_memory=config['data']['pin_memory'],
        audio_params=config['data']['audio_params'],
        augmentation_params=config['data']['augmentation'],
        normalization_params=config['data']['normalization']
    )
    
    # íŠ¸ë ˆì´ë„ˆ ìƒì„± ë° í›ˆë ¨
    trainer = RealArchitectureTrainer(model_type, config, save_dir)
    results = trainer.train(train_loader, val_loader, config['training']['epochs'])
    
    # ì„¤ì • ì €ì¥
    with open(Path(save_dir) / "config.yaml", 'w') as f:
        yaml.dump(config, f, default_flow_style=False)
    
    return results


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="Real Architecture Training Script")
    parser.add_argument("--model", choices=["efficientnet", "melspec_cnn", "both"], 
                       default="both", help="í›ˆë ¨í•  ëª¨ë¸")
    parser.add_argument("--data-root", default="watermelon_sound_data", 
                       help="ë°ì´í„° ë£¨íŠ¸ ë””ë ‰í† ë¦¬")
    
    args = parser.parse_args()
    
    print("ğŸ‰ Real Architecture Training Script!")
    print(f"   ğŸ¯ í›ˆë ¨ ëª¨ë¸: {args.model}")
    print(f"   ğŸ“‚ ë°ì´í„° ê²½ë¡œ: {args.data_root}")
    print()
    
    results = {}
    
    if args.model in ["efficientnet", "both"]:
        print("ğŸš€ EfficientNet ëª¨ë¸ í›ˆë ¨ ì‹œì‘!")
        results['efficientnet'] = train_model('efficientnet', args.data_root)
        print("âœ… EfficientNet í›ˆë ¨ ì™„ë£Œ!\n")
    
    if args.model in ["melspec_cnn", "both"]:
        print("ğŸš€ MelSpecCNN ëª¨ë¸ í›ˆë ¨ ì‹œì‘!")
        results['melspec_cnn'] = train_model('melspec_cnn', args.data_root)
        print("âœ… MelSpecCNN í›ˆë ¨ ì™„ë£Œ!\n")
    
    # ìµœì¢… ê²°ê³¼ ì¶œë ¥
    print("ğŸ‰ ëª¨ë“  í›ˆë ¨ ì™„ë£Œ!")
    for model_name, result in results.items():
        print(f"   {model_name.upper()}: Best Val MAE = {result['best_val_mae']:.4f}")


if __name__ == "__main__":
    main() 