#!/usr/bin/env python3
"""
ì‹¤í—˜ ê²°ê³¼ ë¹„êµ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
"""

import json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import argparse
from typing import Dict, List, Any
import numpy as np

def load_experiment_results(experiments_dir: Path) -> Dict[str, Dict]:
    """
    ì‹¤í—˜ ê²°ê³¼ë“¤ì„ ë¡œë“œ
    
    Args:
        experiments_dir: ì‹¤í—˜ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        
    Returns:
        ì‹¤í—˜ëª…ë³„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    results = {}
    
    # ì‹¤í—˜ ë””ë ‰í† ë¦¬ë“¤ ìŠ¤ìº”
    for exp_dir in experiments_dir.iterdir():
        if exp_dir.is_dir() and not exp_dir.name.startswith('.'):
            metrics_file = exp_dir / "metrics_summary.json"
            success_file = exp_dir / "SUCCESS"
            
            if metrics_file.exists():
                try:
                    with open(metrics_file, 'r') as f:
                        data = json.load(f)
                    
                    # SUCCESS íŒŒì¼ì—ì„œ ìµœì¢… ê²°ê³¼ ì¶”ì¶œ
                    final_results = {}
                    if success_file.exists():
                        with open(success_file, 'r') as f:
                            for line in f:
                                if "ìµœê³  Val MAE:" in line:
                                    final_results['best_val_mae'] = float(line.split(':')[1].strip())
                                elif "ìµœì¢… í…ŒìŠ¤íŠ¸ MAE:" in line:
                                    final_results['final_test_mae'] = float(line.split(':')[1].strip())
                                elif "ìµœì¢… í…ŒìŠ¤íŠ¸ RÂ²:" in line:
                                    final_results['final_test_r2'] = float(line.split(':')[1].strip())
                    
                    # ì‹¤í—˜ëª…ì—ì„œ ì„¤ì • ì¶”ì¶œ
                    exp_name = exp_dir.name
                    config_info = extract_config_from_name(exp_name)
                    
                    results[exp_name] = {
                        'metrics': data,
                        'final_results': final_results,
                        'config': config_info,
                        'status': 'completed' if success_file.exists() else 'in_progress'
                    }
                    
                except Exception as e:
                    print(f"âš ï¸ {exp_dir.name} ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    return results

def extract_config_from_name(exp_name: str) -> Dict[str, Any]:
    """
    ì‹¤í—˜ëª…ì—ì„œ ì„¤ì • ì •ë³´ ì¶”ì¶œ
    
    Args:
        exp_name: ì‹¤í—˜ ë””ë ‰í† ë¦¬ ëª…
        
    Returns:
        ì„¤ì • ì •ë³´ ë”•ì…”ë„ˆë¦¬
    """
    config = {'experiment_type': 'unknown', 'batch_size': 8, 'loss_type': 'mse'}
    
    if 'batch16' in exp_name:
        config['experiment_type'] = 'batch_size'
        config['batch_size'] = 16
    elif 'batch32' in exp_name:
        config['experiment_type'] = 'batch_size'
        config['batch_size'] = 32
    elif 'huber' in exp_name:
        config['experiment_type'] = 'loss_function'
        config['loss_type'] = 'huber'
    elif 'regularized' in exp_name:
        config['experiment_type'] = 'regularization'
        config['regularization'] = 'enhanced'
    elif 'lr_reduced' in exp_name:
        config['experiment_type'] = 'learning_rate'
        config['learning_rate'] = 0.0001
    elif 'baseline' in exp_name:
        config['experiment_type'] = 'baseline'
        config['learning_rate'] = 0.001
    
    return config

def create_comparison_table(results: Dict[str, Dict]) -> pd.DataFrame:
    """
    ì‹¤í—˜ ê²°ê³¼ ë¹„êµ í…Œì´ë¸” ìƒì„±
    
    Args:
        results: ì‹¤í—˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        
    Returns:
        ë¹„êµ í…Œì´ë¸” DataFrame
    """
    data = []
    
    for exp_name, exp_data in results.items():
        if exp_data['status'] == 'completed' and exp_data['final_results']:
            row = {
                'Experiment': exp_name,
                'Type': exp_data['config']['experiment_type'],
                'Batch Size': exp_data['config']['batch_size'],
                'Loss Type': exp_data['config']['loss_type'],
                'Best Val MAE': exp_data['final_results'].get('best_val_mae', 'N/A'),
                'Final Test MAE': exp_data['final_results'].get('final_test_mae', 'N/A'),
                'Final Test RÂ²': exp_data['final_results'].get('final_test_r2', 'N/A'),
                'Status': exp_data['status']
            }
            data.append(row)
    
    df = pd.DataFrame(data)
    return df.sort_values('Best Val MAE') if not df.empty else df

def plot_experiment_comparison(results: Dict[str, Dict], save_path: str = None):
    """
    ì‹¤í—˜ ê²°ê³¼ ì‹œê°í™”
    
    Args:
        results: ì‹¤í—˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        save_path: ì €ì¥ ê²½ë¡œ
    """
    # ì™„ë£Œëœ ì‹¤í—˜ë“¤ë§Œ í•„í„°ë§
    completed_results = {k: v for k, v in results.items() 
                        if v['status'] == 'completed' and v['final_results']}
    
    if not completed_results:
        print("âš ï¸ ì™„ë£Œëœ ì‹¤í—˜ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ë°ì´í„° ì¤€ë¹„
    exp_names = []
    val_maes = []
    test_maes = []
    r2_scores = []
    
    for exp_name, exp_data in completed_results.items():
        exp_names.append(exp_name.replace('_', '\n'))  # ì¤„ë°”ê¿ˆìœ¼ë¡œ ê°€ë…ì„± í–¥ìƒ
        val_maes.append(exp_data['final_results']['best_val_mae'])
        test_maes.append(exp_data['final_results']['final_test_mae'])
        r2_scores.append(exp_data['final_results']['final_test_r2'])
    
    # ì‹œê°í™”
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('ğŸ‰ Watermelon Sweetness Prediction - Experiment Results Comparison', 
                 fontsize=16, fontweight='bold')
    
    # MAE ë¹„êµ
    axes[0, 0].bar(range(len(exp_names)), val_maes, color='skyblue', alpha=0.7)
    axes[0, 0].set_title('Best Validation MAE', fontweight='bold')
    axes[0, 0].set_ylabel('MAE (Brix)')
    axes[0, 0].set_xticks(range(len(exp_names)))
    axes[0, 0].set_xticklabels(exp_names, rotation=45, ha='right')
    
    # Test MAE ë¹„êµ
    axes[0, 1].bar(range(len(exp_names)), test_maes, color='lightcoral', alpha=0.7)
    axes[0, 1].set_title('Final Test MAE', fontweight='bold')
    axes[0, 1].set_ylabel('MAE (Brix)')
    axes[0, 1].set_xticks(range(len(exp_names)))
    axes[0, 1].set_xticklabels(exp_names, rotation=45, ha='right')
    
    # RÂ² Score ë¹„êµ
    axes[1, 0].bar(range(len(exp_names)), r2_scores, color='lightgreen', alpha=0.7)
    axes[1, 0].set_title('Final Test RÂ² Score', fontweight='bold')
    axes[1, 0].set_ylabel('RÂ² Score')
    axes[1, 0].set_xticks(range(len(exp_names)))
    axes[1, 0].set_xticklabels(exp_names, rotation=45, ha='right')
    axes[1, 0].axhline(y=0, color='red', linestyle='--', alpha=0.5, label='RÂ²=0')
    axes[1, 0].legend()
    
    # Val vs Test MAE ì‚°ì ë„
    axes[1, 1].scatter(val_maes, test_maes, s=100, alpha=0.7, color='purple')
    axes[1, 1].set_xlabel('Validation MAE')
    axes[1, 1].set_ylabel('Test MAE')
    axes[1, 1].set_title('Validation vs Test MAE', fontweight='bold')
    
    # ëŒ€ê°ì„  ì¶”ê°€ (ì´ìƒì ì¸ ê²½ìš°)
    min_mae = min(min(val_maes), min(test_maes))
    max_mae = max(max(val_maes), max(test_maes))
    axes[1, 1].plot([min_mae, max_mae], [min_mae, max_mae], 'r--', alpha=0.5, label='Perfect Fit')
    axes[1, 1].legend()
    
    # ì‹¤í—˜ëª… ë¼ë²¨ ì¶”ê°€
    for i, name in enumerate(exp_names):
        axes[1, 1].annotate(name.replace('\n', '_'), (val_maes[i], test_maes[i]), 
                           xytext=(5, 5), textcoords='offset points', fontsize=8)
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ğŸ“Š ë¹„êµ ì°¨íŠ¸ ì €ì¥ë¨: {save_path}")
    
    plt.show()

def generate_experiment_report(results: Dict[str, Dict], save_path: str = None) -> str:
    """
    ì‹¤í—˜ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±
    
    Args:
        results: ì‹¤í—˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        save_path: ì €ì¥ ê²½ë¡œ
        
    Returns:
        ë¦¬í¬íŠ¸ í…ìŠ¤íŠ¸
    """
    report = []
    report.append("# ğŸ‰ Watermelon Sweetness Prediction - Experiment Results Report")
    report.append("=" * 70)
    report.append("")
    
    # ì™„ë£Œëœ ì‹¤í—˜ ìš”ì•½
    completed = [k for k, v in results.items() if v['status'] == 'completed']
    in_progress = [k for k, v in results.items() if v['status'] == 'in_progress']
    
    report.append(f"## ğŸ“Š ì‹¤í—˜ í˜„í™©")
    report.append(f"- âœ… ì™„ë£Œëœ ì‹¤í—˜: {len(completed)}ê°œ")
    report.append(f"- ğŸ”„ ì§„í–‰ ì¤‘ì¸ ì‹¤í—˜: {len(in_progress)}ê°œ")
    report.append("")
    
    # ìµœê³  ì„±ëŠ¥ ì‹¤í—˜
    if completed:
        best_exp = min(completed, key=lambda x: results[x]['final_results'].get('best_val_mae', float('inf')))
        best_mae = results[best_exp]['final_results']['best_val_mae']
        
        report.append(f"## ğŸ† ìµœê³  ì„±ëŠ¥ ì‹¤í—˜")
        report.append(f"- **ì‹¤í—˜ëª…**: {best_exp}")
        report.append(f"- **Best Val MAE**: {best_mae:.4f}")
        report.append(f"- **ì‹¤í—˜ íƒ€ì…**: {results[best_exp]['config']['experiment_type']}")
        report.append("")
    
    # ì‹¤í—˜ë³„ ìƒì„¸ ê²°ê³¼
    report.append("## ğŸ“‹ ì‹¤í—˜ë³„ ìƒì„¸ ê²°ê³¼")
    
    for exp_name, exp_data in results.items():
        if exp_data['status'] == 'completed' and exp_data['final_results']:
            report.append(f"### {exp_name}")
            report.append(f"- **Status**: {exp_data['status']}")
            report.append(f"- **Type**: {exp_data['config']['experiment_type']}")
            report.append(f"- **Best Val MAE**: {exp_data['final_results']['best_val_mae']:.4f}")
            report.append(f"- **Final Test MAE**: {exp_data['final_results']['final_test_mae']:.4f}")
            report.append(f"- **Final Test RÂ²**: {exp_data['final_results']['final_test_r2']:.4f}")
            report.append("")
    
    # ì§„í–‰ ì¤‘ì¸ ì‹¤í—˜
    if in_progress:
        report.append("## ğŸ”„ ì§„í–‰ ì¤‘ì¸ ì‹¤í—˜")
        for exp_name in in_progress:
            report.append(f"- {exp_name}: {results[exp_name]['config']['experiment_type']}")
        report.append("")
    
    report_text = "\n".join(report)
    
    if save_path:
        with open(save_path, 'w', encoding='utf-8') as f:
            f.write(report_text)
        print(f"ğŸ“„ ë¦¬í¬íŠ¸ ì €ì¥ë¨: {save_path}")
    
    return report_text

def main():
    parser = argparse.ArgumentParser(description="ì‹¤í—˜ ê²°ê³¼ ë¹„êµ ë¶„ì„")
    parser.add_argument("--experiments-dir", type=str, default="experiments", 
                       help="ì‹¤í—˜ ë””ë ‰í† ë¦¬ ê²½ë¡œ")
    parser.add_argument("--output-dir", type=str, default="experiments/comparison", 
                       help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--show-plot", action="store_true", help="ì°¨íŠ¸ í‘œì‹œ")
    
    args = parser.parse_args()
    
    # ê²½ë¡œ ì„¤ì •
    experiments_dir = Path(args.experiments_dir)
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("ğŸ” ì‹¤í—˜ ê²°ê³¼ ë¡œë”© ì¤‘...")
    results = load_experiment_results(experiments_dir)
    
    if not results:
        print("âŒ ì‹¤í—˜ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“Š {len(results)}ê°œ ì‹¤í—˜ ë°œê²¬")
    
    # ë¹„êµ í…Œì´ë¸” ìƒì„±
    df = create_comparison_table(results)
    if not df.empty:
        table_path = output_dir / "experiment_comparison.csv"
        df.to_csv(table_path, index=False)
        print(f"ğŸ“Š ë¹„êµ í…Œì´ë¸” ì €ì¥ë¨: {table_path}")
        print("\n" + df.to_string(index=False))
    
    # ì‹œê°í™”
    chart_path = output_dir / "experiment_comparison.png"
    plot_experiment_comparison(results, str(chart_path))
    
    # ë¦¬í¬íŠ¸ ìƒì„±
    report_path = output_dir / "experiment_report.md"
    report = generate_experiment_report(results, str(report_path))
    print("\n" + "="*50)
    print(report)

if __name__ == "__main__":
    main() 