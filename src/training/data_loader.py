"""
Data Loader Module
ìˆ˜ë°• ë‹¹ë„ ì˜ˆì¸¡ì„ ìœ„í•œ ë°ì´í„° ë¡œë” ëª¨ë“ˆ
"""

import torch
from torch.utils.data import DataLoader, Subset
from typing import Tuple, Dict, Any, Optional, List
import yaml
from pathlib import Path

# ìƒëŒ€ import 
import sys
from pathlib import Path
current_dir = Path(__file__).parent
src_dir = current_dir.parent
sys.path.insert(0, str(src_dir))

from data.watermelon_dataset import WatermelonDataset
from data.data_splitter import create_data_splits
from data.augmentation import RandomAudioAugmentation


class WatermelonDataLoader:
    """
    ìˆ˜ë°• ë°ì´í„°ì…‹ì„ ìœ„í•œ ë°ì´í„° ë¡œë” í´ë˜ìŠ¤
    """
    
    def __init__(self, 
                 train_dataset: WatermelonDataset,
                 val_dataset: WatermelonDataset,
                 test_dataset: WatermelonDataset,
                 batch_size: int = 32,
                 num_workers: int = 4,
                 pin_memory: bool = True):
        """
        WatermelonDataLoader ì´ˆê¸°í™”
        
        Args:
            train_dataset (WatermelonDataset): í›ˆë ¨ ë°ì´í„°ì…‹
            val_dataset (WatermelonDataset): ê²€ì¦ ë°ì´í„°ì…‹
            test_dataset (WatermelonDataset): í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹
            batch_size (int): ë°°ì¹˜ í¬ê¸°
            num_workers (int): ë°ì´í„° ë¡œë”© ì›Œì»¤ ìˆ˜
            pin_memory (bool): GPU ë©”ëª¨ë¦¬ ê³ ì • ì—¬ë¶€
        """
        self.train_dataset = train_dataset
        self.val_dataset = val_dataset
        self.test_dataset = test_dataset
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.pin_memory = pin_memory
        
        # ë°ì´í„° ë¡œë” ìƒì„±
        self.train_loader = self._create_train_loader()
        self.val_loader = self._create_val_loader()
        self.test_loader = self._create_test_loader()
    
    def _create_train_loader(self) -> DataLoader:
        """í›ˆë ¨ìš© ë°ì´í„° ë¡œë” ìƒì„±"""
        return DataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            shuffle=True,  # í›ˆë ¨ ì‹œ ì…”í”Œ
            num_workers=self.num_workers,
            pin_memory=self.pin_memory,
            drop_last=True,  # ë§ˆì§€ë§‰ ë¶ˆì™„ì „í•œ ë°°ì¹˜ ì œê±°
            persistent_workers=self.num_workers > 0
        )
    
    def _create_val_loader(self) -> DataLoader:
        """ê²€ì¦ìš© ë°ì´í„° ë¡œë” ìƒì„±"""
        return DataLoader(
            self.val_dataset,
            batch_size=self.batch_size,
            shuffle=False,  # ê²€ì¦ ì‹œ ìˆœì„œ ìœ ì§€
            num_workers=self.num_workers,
            pin_memory=self.pin_memory,
            drop_last=False,
            persistent_workers=self.num_workers > 0
        )
    
    def _create_test_loader(self) -> DataLoader:
        """í…ŒìŠ¤íŠ¸ìš© ë°ì´í„° ë¡œë” ìƒì„±"""
        return DataLoader(
            self.test_dataset,
            batch_size=self.batch_size,
            shuffle=False,  # í…ŒìŠ¤íŠ¸ ì‹œ ìˆœì„œ ìœ ì§€
            num_workers=self.num_workers,
            pin_memory=self.pin_memory,
            drop_last=False,
            persistent_workers=self.num_workers > 0
        )
    
    def get_data_info(self) -> Dict[str, Any]:
        """ë°ì´í„° ë¡œë” ì •ë³´ ë°˜í™˜"""
        return {
            'train_size': len(self.train_dataset),
            'val_size': len(self.val_dataset),
            'test_size': len(self.test_dataset),
            'batch_size': self.batch_size,
            'num_workers': self.num_workers,
            'train_batches': len(self.train_loader),
            'val_batches': len(self.val_loader),
            'test_batches': len(self.test_loader),
            'pin_memory': self.pin_memory
        }
    
    def print_data_info(self):
        """ë°ì´í„° ë¡œë” ì •ë³´ ì¶œë ¥"""
        info = self.get_data_info()
        print(f"ğŸ“Š ë°ì´í„° ë¡œë” ì •ë³´")
        print(f"   ğŸš‚ Train: {info['train_size']:,}ê°œ ìƒ˜í”Œ, {info['train_batches']:,}ê°œ ë°°ì¹˜")
        print(f"   ğŸ” Val: {info['val_size']:,}ê°œ ìƒ˜í”Œ, {info['val_batches']:,}ê°œ ë°°ì¹˜")
        print(f"   ğŸ§ª Test: {info['test_size']:,}ê°œ ìƒ˜í”Œ, {info['test_batches']:,}ê°œ ë°°ì¹˜")
        print(f"   ğŸ“¦ ë°°ì¹˜ í¬ê¸°: {info['batch_size']}")
        print(f"   ğŸ‘· ì›Œì»¤ ìˆ˜: {info['num_workers']}")
        print(f"   ğŸ“Œ ë©”ëª¨ë¦¬ ê³ ì •: {info['pin_memory']}")


def create_data_loaders(data_path: str,
                       train_ratio: float = 0.7,
                       val_ratio: float = 0.15,
                       test_ratio: float = 0.15,
                       batch_size: int = 32,
                       num_workers: int = 4,
                       pin_memory: bool = True,
                       use_augmentation: bool = True,
                       stratify_by_sweetness: bool = True,
                       random_seed: int = 42,
                       split_file: Optional[str] = None,
                       config_path: Optional[str] = None) -> WatermelonDataLoader:
    """
    ë°ì´í„° ë¡œë” ìƒì„± í•¨ìˆ˜
    
    Args:
        data_path (str): ë°ì´í„° ê²½ë¡œ
        train_ratio (float): í›ˆë ¨ ë°ì´í„° ë¹„ìœ¨
        val_ratio (float): ê²€ì¦ ë°ì´í„° ë¹„ìœ¨
        test_ratio (float): í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¹„ìœ¨
        batch_size (int): ë°°ì¹˜ í¬ê¸°
        num_workers (int): ì›Œì»¤ ìˆ˜
        pin_memory (bool): ë©”ëª¨ë¦¬ ê³ ì • ì—¬ë¶€
        use_augmentation (bool): ë°ì´í„° ì¦ê°• ì‚¬ìš© ì—¬ë¶€
        stratify_by_sweetness (bool): ë‹¹ë„ë³„ ì¸µí™” ë¶„í•  ì—¬ë¶€
        random_seed (int): ëœë¤ ì‹œë“œ
        split_file (str, optional): ê¸°ì¡´ ë¶„í•  íŒŒì¼ ê²½ë¡œ
        config_path (str, optional): ì„¤ì • íŒŒì¼ ê²½ë¡œ
        
    Returns:
        WatermelonDataLoader: ìƒì„±ëœ ë°ì´í„° ë¡œë”
    """
    # ì„¤ì • íŒŒì¼ì—ì„œ íŒŒë¼ë¯¸í„° ë¡œë“œ
    if config_path:
        config_file = Path(config_path)
        if config_file.exists():
            with open(config_file, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
                data_config = config.get('data', {})
                
                # ì„¤ì • ê°’ìœ¼ë¡œ ë®ì–´ì“°ê¸° (í•¨ìˆ˜ ì¸ìê°€ ìš°ì„ )
                batch_size = data_config.get('batch_size', batch_size)
                num_workers = data_config.get('num_workers', num_workers)
                pin_memory = data_config.get('pin_memory', pin_memory)
    
    print(f"ğŸ‰ ë°ì´í„° ë¡œë” ìƒì„± ì‹œì‘")
    print(f"   ğŸ“ ë°ì´í„° ê²½ë¡œ: {data_path}")
    
    # ì›ë³¸ ë°ì´í„°ì…‹ ìƒì„±
    dataset = WatermelonDataset(data_path)
    print(f"   ğŸ“Š ì „ì²´ ë°ì´í„°: {len(dataset)}ê°œ ìƒ˜í”Œ")
    
    # ë°ì´í„° ì¦ê°• ì„¤ì •
    if use_augmentation:
        augmentation_config = {
            'add_noise': {'snr_db': (15.0, 25.0), 'noise_type': 'white'},
            'time_shift': {'shift_limit': 0.2},
            'pitch_shift': {'step_range': 2.0},
            'volume_scaling': {'scale_range': (0.8, 1.2)},
            'time_stretch': {'rate_range': (0.9, 1.1)},
            'add_reverb': {'room_range': (0.1, 0.3)}
        }
        augmentation = RandomAudioAugmentation(
            augmentation_config=augmentation_config,
            probability=0.3
        )
        # Note: WatermelonDataset needs to support augmentation in __getitem__
        print(f"   ğŸ­ ë°ì´í„° ì¦ê°• ì¤€ë¹„ ì™„ë£Œ (ë°ì´í„°ì…‹ì—ì„œ ì ìš© í•„ìš”)")
    
    # ë°ì´í„° ë¶„í• 
    if split_file and Path(split_file).exists():
        # ê¸°ì¡´ ë¶„í•  íŒŒì¼ ì‚¬ìš©
        from data.data_splitter import DataSplitter
        train_dataset, val_dataset, test_dataset = DataSplitter.create_datasets_from_split_file(
            dataset, split_file
        )
        print(f"   ğŸ“‚ ê¸°ì¡´ ë¶„í•  íŒŒì¼ ì‚¬ìš©: {split_file}")
    else:
        # ìƒˆë¡œìš´ ë¶„í•  ìƒì„±
        train_dataset, val_dataset, test_dataset = create_data_splits(
            dataset,
            train_ratio=train_ratio,
            val_ratio=val_ratio,
            test_ratio=test_ratio,
            stratify_by_sweetness=stratify_by_sweetness,
            random_seed=random_seed,
            save_split_info=split_file
        )
    
    # ë°ì´í„° ë¡œë” ìƒì„±
    data_loader = WatermelonDataLoader(
        train_dataset=train_dataset,
        val_dataset=val_dataset,
        test_dataset=test_dataset,
        batch_size=batch_size,
        num_workers=num_workers,
        pin_memory=pin_memory
    )
    
    data_loader.print_data_info()
    print(f"âœ… ë°ì´í„° ë¡œë” ìƒì„± ì™„ë£Œ")
    
    return data_loader


def create_single_dataloader(dataset: WatermelonDataset,
                           batch_size: int = 32,
                           shuffle: bool = False,
                           num_workers: int = 4,
                           pin_memory: bool = True,
                           drop_last: bool = False) -> DataLoader:
    """
    ë‹¨ì¼ ë°ì´í„°ì…‹ì„ ìœ„í•œ ë°ì´í„° ë¡œë” ìƒì„±
    
    Args:
        dataset (WatermelonDataset): ë°ì´í„°ì…‹
        batch_size (int): ë°°ì¹˜ í¬ê¸°
        shuffle (bool): ì…”í”Œ ì—¬ë¶€
        num_workers (int): ì›Œì»¤ ìˆ˜
        pin_memory (bool): ë©”ëª¨ë¦¬ ê³ ì • ì—¬ë¶€
        drop_last (bool): ë§ˆì§€ë§‰ ë°°ì¹˜ ì œê±° ì—¬ë¶€
        
    Returns:
        DataLoader: ìƒì„±ëœ ë°ì´í„° ë¡œë”
    """
    return DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        num_workers=num_workers,
        pin_memory=pin_memory,
        drop_last=drop_last,
        persistent_workers=num_workers > 0
    )


def get_sample_batch(data_loader: DataLoader, device: torch.device) -> Tuple[torch.Tensor, torch.Tensor]:
    """
    ë°ì´í„° ë¡œë”ì—ì„œ ìƒ˜í”Œ ë°°ì¹˜ ì¶”ì¶œ
    
    Args:
        data_loader (DataLoader): ë°ì´í„° ë¡œë”
        device (torch.device): ë””ë°”ì´ìŠ¤
        
    Returns:
        Tuple[torch.Tensor, torch.Tensor]: (ì…ë ¥, íƒ€ê²Ÿ) í…ì„œ
    """
    data_iter = iter(data_loader)
    inputs, targets = next(data_iter)
    
    return inputs.to(device), targets.to(device)


def calculate_dataset_stats(data_loader: DataLoader) -> Dict[str, float]:
    """
    ë°ì´í„°ì…‹ í†µê³„ ê³„ì‚°
    
    Args:
        data_loader (DataLoader): ë°ì´í„° ë¡œë”
        
    Returns:
        Dict[str, float]: ë°ì´í„°ì…‹ í†µê³„
    """
    print("ğŸ“ˆ ë°ì´í„°ì…‹ í†µê³„ ê³„ì‚° ì¤‘...")
    
    all_targets = []
    total_samples = 0
    
    for _, targets in data_loader:
        all_targets.append(targets)
        total_samples += targets.size(0)
    
    all_targets = torch.cat(all_targets, dim=0)
    
    stats = {
        'mean': float(all_targets.mean()),
        'std': float(all_targets.std()),
        'min': float(all_targets.min()),
        'max': float(all_targets.max()),
        'median': float(all_targets.median()),
        'total_samples': total_samples
    }
    
    print(f"   ğŸ“Š í‰ê· : {stats['mean']:.2f} Â± {stats['std']:.2f}")
    print(f"   ğŸ“ ë²”ìœ„: {stats['min']:.1f} ~ {stats['max']:.1f}")
    print(f"   ğŸ“ ì¤‘ì•™ê°’: {stats['median']:.2f}")
    print(f"   ğŸ”¢ ì´ ìƒ˜í”Œ: {stats['total_samples']:,}")
    
    return stats


if __name__ == "__main__":
    # ë°ì´í„° ë¡œë” í…ŒìŠ¤íŠ¸
    print("ğŸ§ª ë°ì´í„° ë¡œë” í…ŒìŠ¤íŠ¸")
    
    # ë°ì´í„° ê²½ë¡œ ì„¤ì •
    data_path = "../../watermelon_sound_data"
    
    try:
        # ë°ì´í„° ë¡œë” ìƒì„±
        data_loader = create_data_loaders(
            data_path=data_path,
            batch_size=16,
            num_workers=2,
            use_augmentation=True
        )
        
        # ìƒ˜í”Œ ë°°ì¹˜ í…ŒìŠ¤íŠ¸
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸ“± ë””ë°”ì´ìŠ¤: {device}")
        
        # í›ˆë ¨ ë°ì´í„° ìƒ˜í”Œ
        train_inputs, train_targets = get_sample_batch(data_loader.train_loader, device)
        print(f"ğŸš‚ í›ˆë ¨ ë°°ì¹˜: ì…ë ¥ {train_inputs.shape}, íƒ€ê²Ÿ {train_targets.shape}")
        
        # í†µê³„ ê³„ì‚°
        train_stats = calculate_dataset_stats(data_loader.train_loader)
        
        print("âœ… ë°ì´í„° ë¡œë” í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ ë°ì´í„° ë¡œë” í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc() 