#!/usr/bin/env python3
"""
Direct Model Training Script
EfficientNetê³¼ MelSpecCNNì„ ì§ì ‘ í›ˆë ¨í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ (VGG ìš°íšŒ)

# EfficientNet vs MelSpecCNN ì„±ëŠ¥ ë¹„êµ (ê¶Œì¥)
python scripts/train_direct_models.py --model compare

# ê°œë³„ ëª¨ë¸ í›ˆë ¨
python scripts/train_direct_models.py --model efficientnet
python scripts/train_direct_models.py --model melspec_cnn
"""

import os
import sys
import argparse
import yaml
import torch
import torch.nn as nn
import torch.optim as optim
from pathlib import Path
from datetime import datetime
import numpy as np
from tqdm import tqdm

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.models.efficientnet_watermelon import create_efficientnet_watermelon
from src.models.melspec_cnn_watermelon import create_melspec_cnn_watermelon
from src.training.data_loader import create_data_loaders


class DirectModelTrainer:
    """ì§ì ‘ ëª¨ë¸ í›ˆë ¨ í´ë˜ìŠ¤"""
    
    def __init__(self, model, data_loader, save_dir, device='cpu'):
        self.device = torch.device(device) if isinstance(device, str) else device
        self.model = model.to(self.device)
        self.data_loader = data_loader
        self.save_dir = Path(save_dir)
        
        # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
        # ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € ì„¤ì •
        self.criterion = nn.MSELoss()
        self.optimizer = optim.Adam(
            self.model.parameters(),
            lr=0.0001,
            weight_decay=1e-4
        )
        self.scheduler = optim.lr_scheduler.StepLR(
            self.optimizer,
            step_size=10,
            gamma=0.7
        )
        
        # í›ˆë ¨ ê¸°ë¡
        self.train_losses = []
        self.val_losses = []
        self.val_maes = []
        self.best_val_loss = float('inf')
        self.best_val_mae = float('inf')
        
    def train_epoch(self, epoch):
        """í•œ ì—í­ í›ˆë ¨"""
        self.model.train()
        running_loss = 0.0
        num_batches = 0
        
        progress_bar = tqdm(
            self.data_loader.train_loader,
            desc=f"Epoch {epoch+1} í›ˆë ¨",
            leave=False
        )
        
        for batch_idx, (inputs, targets) in enumerate(progress_bar):
            inputs, targets = inputs.to(self.device), targets.to(self.device)
            
            # Forward pass
            self.optimizer.zero_grad()
            outputs = self.model(inputs)
            loss = self.criterion(outputs, targets.unsqueeze(1))
            
            # Backward pass
            loss.backward()
            self.optimizer.step()
            
            running_loss += loss.item()
            num_batches += 1
            
            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
            progress_bar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Avg': f'{running_loss/num_batches:.4f}'
            })
        
        avg_train_loss = running_loss / num_batches
        self.train_losses.append(avg_train_loss)
        
        return avg_train_loss
    
    def validate_epoch(self, epoch):
        """í•œ ì—í­ ê²€ì¦"""
        self.model.eval()
        running_loss = 0.0
        all_predictions = []
        all_targets = []
        
        with torch.no_grad():
            for inputs, targets in self.data_loader.val_loader:
                inputs, targets = inputs.to(self.device), targets.to(self.device)
                
                outputs = self.model(inputs)
                loss = self.criterion(outputs, targets.unsqueeze(1))
                
                running_loss += loss.item()
                all_predictions.extend(outputs.cpu().numpy().flatten())
                all_targets.extend(targets.cpu().numpy())
        
        avg_val_loss = running_loss / len(self.data_loader.val_loader)
        self.val_losses.append(avg_val_loss)
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        val_mae = np.mean(np.abs(np.array(all_predictions) - np.array(all_targets)))
        self.val_maes.append(val_mae)
        
        # ìµœê³  ì„±ëŠ¥ ì—…ë°ì´íŠ¸
        if avg_val_loss < self.best_val_loss:
            self.best_val_loss = avg_val_loss
            self.save_checkpoint(epoch, 'best_loss')
        
        if val_mae < self.best_val_mae:
            self.best_val_mae = val_mae
            self.save_checkpoint(epoch, 'best_mae')
        
        return avg_val_loss, val_mae
    
    def save_checkpoint(self, epoch, name):
        """ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'best_val_loss': self.best_val_loss,
            'best_val_mae': self.best_val_mae,
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'val_maes': self.val_maes
        }
        
        save_path = self.save_dir / f"{name}_epoch_{epoch+1}.pth"
        torch.save(checkpoint, save_path)
        print(f"   ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {save_path}")
    
    def train(self, num_epochs=30):
        """ì „ì²´ í›ˆë ¨ ê³¼ì •"""
        print(f"ğŸš€ í›ˆë ¨ ì‹œì‘! (ì´ {num_epochs} ì—í­)")
        print(f"   ğŸ–¥ï¸ ë””ë°”ì´ìŠ¤: {self.device}")
        
        for epoch in range(num_epochs):
            # í›ˆë ¨
            train_loss = self.train_epoch(epoch)
            
            # ê²€ì¦
            val_loss, val_mae = self.validate_epoch(epoch)
            
            # í•™ìŠµë¥  ì¡°ì •
            self.scheduler.step()
            current_lr = self.optimizer.param_groups[0]['lr']
            
            # ê²°ê³¼ ì¶œë ¥
            print(f"Epoch {epoch+1:3d}/{num_epochs}: "
                  f"Train Loss: {train_loss:.4f}, "
                  f"Val Loss: {val_loss:.4f}, "
                  f"Val MAE: {val_mae:.4f}, "
                  f"LR: {current_lr:.6f}")
            
            # Early stopping (ê°„ë‹¨í•œ ë²„ì „)
            if epoch > 10 and val_loss > self.best_val_loss * 1.5:
                print("   â¹ï¸ Early stopping triggered")
                break
        
        print(f"âœ… í›ˆë ¨ ì™„ë£Œ!")
        print(f"   ğŸ† ìµœê³  Val Loss: {self.best_val_loss:.4f}")
        print(f"   ğŸ† ìµœê³  Val MAE: {self.best_val_mae:.4f}")
        
        return {
            'best_val_loss': self.best_val_loss,
            'best_val_mae': self.best_val_mae,
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'val_maes': self.val_maes
        }


def train_efficientnet(data_path="watermelon_sound_data"):
    """EfficientNet ëª¨ë¸ í›ˆë ¨"""
    print("ğŸš€ EfficientNet ëª¨ë¸ ì§ì ‘ í›ˆë ¨ ì‹œì‘!")
    
    # ëª¨ë¸ ìƒì„±
    print("ğŸ”§ EfficientNet ëª¨ë¸ ìƒì„± ì¤‘...")
    model = create_efficientnet_watermelon(
        pretrained=False,
        dropout_rate=0.7,
        num_fc_layers=2,
        fc_hidden_size=256
    )
    model.print_model_info()
    
    # ë°ì´í„° ë¡œë” ìƒì„±
    print("ğŸ“‚ ë°ì´í„° ë¡œë” ìƒì„± ì¤‘...")
    data_loader = create_data_loaders(
        data_path=data_path,
        batch_size=16,
        num_workers=4,
        pin_memory=True,
        use_augmentation=True,
        stratify_by_sweetness=True,
        random_seed=42
    )
    
    # ì €ì¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    save_dir = f"experiments/direct_efficientnet_{timestamp}"
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # íŠ¸ë ˆì´ë„ˆ ìƒì„± ë° í›ˆë ¨
    trainer = DirectModelTrainer(model, data_loader, save_dir, str(device))
    results = trainer.train(num_epochs=30)
    
    return results, save_dir


def train_melspec_cnn(data_path="watermelon_sound_data"):
    """MelSpecCNN ëª¨ë¸ í›ˆë ¨"""
    print("ğŸš€ MelSpecCNN ëª¨ë¸ ì§ì ‘ í›ˆë ¨ ì‹œì‘!")
    
    # ëª¨ë¸ ìƒì„±
    print("ğŸ”§ MelSpecCNN ëª¨ë¸ ìƒì„± ì¤‘...")
    model = create_melspec_cnn_watermelon(
        input_channels=3,
        base_channels=32,
        dropout_rate=0.7
    )
    model.print_model_info()
    
    # ë°ì´í„° ë¡œë” ìƒì„±
    print("ğŸ“‚ ë°ì´í„° ë¡œë” ìƒì„± ì¤‘...")
    data_loader = create_data_loaders(
        data_path=data_path,
        batch_size=16,
        num_workers=4,
        pin_memory=True,
        use_augmentation=True,
        stratify_by_sweetness=True,
        random_seed=42
    )
    
    # ì €ì¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    save_dir = f"experiments/direct_melspec_cnn_{timestamp}"
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # íŠ¸ë ˆì´ë„ˆ ìƒì„± ë° í›ˆë ¨
    trainer = DirectModelTrainer(model, data_loader, save_dir, str(device))
    results = trainer.train(num_epochs=30)
    
    return results, save_dir


def compare_models(data_path="watermelon_sound_data"):
    """ë‘ ëª¨ë¸ì„ í›ˆë ¨í•˜ê³  ì„±ëŠ¥ ë¹„êµ"""
    print("ğŸ‰ EfficientNet vs MelSpecCNN ì„±ëŠ¥ ë¹„êµ ì‹œì‘!")
    print("="*60)
    
    # EfficientNet í›ˆë ¨
    print("\n1ï¸âƒ£ EfficientNet í›ˆë ¨")
    print("-"*40)
    efficientnet_results, efficientnet_dir = train_efficientnet(data_path)
    
    print("\n2ï¸âƒ£ MelSpecCNN í›ˆë ¨")
    print("-"*40)
    melspec_results, melspec_dir = train_melspec_cnn(data_path)
    
    # ê²°ê³¼ ë¹„êµ
    print("\nğŸ“Š ì„±ëŠ¥ ë¹„êµ ê²°ê³¼")
    print("="*60)
    print(f"ğŸ”¸ EfficientNet:")
    print(f"   ìµœê³  Val Loss: {efficientnet_results['best_val_loss']:.4f}")
    print(f"   ìµœê³  Val MAE:  {efficientnet_results['best_val_mae']:.4f}")
    print(f"   ì €ì¥ ê²½ë¡œ: {efficientnet_dir}")
    
    print(f"\nğŸ”¸ MelSpecCNN:")
    print(f"   ìµœê³  Val Loss: {melspec_results['best_val_loss']:.4f}")
    print(f"   ìµœê³  Val MAE:  {melspec_results['best_val_mae']:.4f}")
    print(f"   ì €ì¥ ê²½ë¡œ: {melspec_dir}")
    
    # ìŠ¹ì ê²°ì •
    if efficientnet_results['best_val_mae'] < melspec_results['best_val_mae']:
        print(f"\nğŸ† ìŠ¹ì: EfficientNet (MAE ì°¨ì´: {melspec_results['best_val_mae'] - efficientnet_results['best_val_mae']:.4f})")
    else:
        print(f"\nğŸ† ìŠ¹ì: MelSpecCNN (MAE ì°¨ì´: {efficientnet_results['best_val_mae'] - melspec_results['best_val_mae']:.4f})")
    
    return efficientnet_results, melspec_results


def main():
    parser = argparse.ArgumentParser(description="Direct Model Training")
    parser.add_argument('--model', 
                       choices=['efficientnet', 'melspec_cnn', 'compare'], 
                       default='compare',
                       help='í›ˆë ¨í•  ëª¨ë¸')
    parser.add_argument('--data', 
                       default='watermelon_sound_data',
                       help='ë°ì´í„° ê²½ë¡œ')
    
    args = parser.parse_args()
    
    if args.model == 'efficientnet':
        train_efficientnet(args.data)
    elif args.model == 'melspec_cnn':
        train_melspec_cnn(args.data)
    elif args.model == 'compare':
        compare_models(args.data)


if __name__ == "__main__":
    main() 