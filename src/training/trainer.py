"""
Trainer Module
ìˆ˜ë°• ë‹¹ë„ ì˜ˆì¸¡ ëª¨ë¸ í›ˆë ¨ì„ ìœ„í•œ í†µí•© íŠ¸ë ˆì´ë„ˆ ëª¨ë“ˆ
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torch.optim.lr_scheduler import ReduceLROnPlateau, StepLR, CosineAnnealingLR
from typing import Dict, Any, Optional, List, Tuple
import yaml
from pathlib import Path
import time
import numpy as np
from tqdm import tqdm
import warnings

# ìƒëŒ€ import
import sys
from pathlib import Path
current_dir = Path(__file__).parent
src_dir = current_dir.parent
sys.path.insert(0, str(src_dir))

from models.vgg_watermelon import VGGWatermelon, save_model_checkpoint
from training.data_loader import WatermelonDataLoader
from training.loss_functions import create_loss_function
from training.metrics import RegressionMetrics, MetricsTracker


class WatermelonTrainer:
    """
    ìˆ˜ë°• ë‹¹ë„ ì˜ˆì¸¡ ëª¨ë¸ í›ˆë ¨ì„ ìœ„í•œ í†µí•© íŠ¸ë ˆì´ë„ˆ í´ë˜ìŠ¤
    """
    
    def __init__(self, 
                 model: VGGWatermelon,
                 data_loader: WatermelonDataLoader,
                 loss_function: nn.Module,
                 optimizer: optim.Optimizer,
                 device: torch.device,
                 scheduler: Optional[optim.lr_scheduler._LRScheduler] = None,
                 save_dir: str = "experiments",
                 experiment_name: str = "watermelon_training"):
        """
        WatermelonTrainer ì´ˆê¸°í™”
        
        Args:
            model (VGGWatermelon): í›ˆë ¨í•  ëª¨ë¸
            data_loader (WatermelonDataLoader): ë°ì´í„° ë¡œë”
            loss_function (nn.Module): ì†ì‹¤ í•¨ìˆ˜
            optimizer (optim.Optimizer): ì˜µí‹°ë§ˆì´ì €
            device (torch.device): ë””ë°”ì´ìŠ¤
            scheduler (optim.lr_scheduler._LRScheduler, optional): í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
            save_dir (str): ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬
            experiment_name (str): ì‹¤í—˜ ì´ë¦„
        """
        self.model = model.to(device)
        self.data_loader = data_loader
        self.loss_function = loss_function
        self.optimizer = optimizer
        self.device = device
        self.scheduler = scheduler
        
        # ì €ì¥ ê²½ë¡œ ì„¤ì •
        self.save_dir = Path(save_dir) / experiment_name
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
        # ë©”íŠ¸ë¦­ ì¶”ì 
        self.metrics_tracker = MetricsTracker()
        
        # í›ˆë ¨ ìƒíƒœ
        self.current_epoch = 0
        self.best_val_mae = float('inf')
        self.best_model_path = self.save_dir / "best_model.pth"
        self.last_model_path = self.save_dir / "last_model.pth"
        
        # Early stopping
        self.early_stopping_patience = 10
        self.early_stopping_counter = 0
        
        print(f"ğŸš€ WatermelonTrainer ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   ğŸ’¾ ì €ì¥ ê²½ë¡œ: {self.save_dir}")
        print(f"   ğŸ–¥ï¸ ë””ë°”ì´ìŠ¤: {self.device}")
    
    def train_epoch(self) -> Dict[str, float]:
        """
        í•œ ì—í¬í¬ í›ˆë ¨
        
        Returns:
            Dict[str, float]: í›ˆë ¨ ë©”íŠ¸ë¦­
        """
        self.model.train()
        
        # ë©”íŠ¸ë¦­ ì´ˆê¸°í™”
        train_metrics = RegressionMetrics(self.device)
        epoch_loss = 0.0
        num_batches = 0
        
        # ì§„í–‰ ë°” ì„¤ì •
        pbar = tqdm(self.data_loader.train_loader, 
                   desc=f"Epoch {self.current_epoch+1} [Train]",
                   leave=False)
        
        for batch_idx, (inputs, targets) in enumerate(pbar):
            # ë°ì´í„°ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            inputs = inputs.to(self.device)
            targets = targets.to(self.device)
            
            # ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”
            self.optimizer.zero_grad()
            
            # ìˆœì „íŒŒ
            predictions = self.model(inputs)
            
            # ì†ì‹¤ ê³„ì‚°
            loss = self.loss_function(predictions, targets)
            
            # ì—­ì „íŒŒ
            loss.backward()
            
            # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ (ì„ íƒì )
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            
            # ì˜µí‹°ë§ˆì´ì € ìŠ¤í…
            self.optimizer.step()
            
            # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            train_metrics.update(predictions, targets, loss)
            epoch_loss += loss.item()
            num_batches += 1
            
            # ì§„í–‰ ë°” ì—…ë°ì´íŠ¸
            pbar.set_postfix({
                'Loss': f"{loss.item():.4f}",
                'Avg Loss': f"{epoch_loss/num_batches:.4f}"
            })
        
        # í›ˆë ¨ ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics = train_metrics.compute()
        metrics['avg_loss'] = epoch_loss / num_batches
        
        return metrics
    
    def validate_epoch(self) -> Dict[str, float]:
        """
        í•œ ì—í¬í¬ ê²€ì¦
        
        Returns:
            Dict[str, float]: ê²€ì¦ ë©”íŠ¸ë¦­
        """
        self.model.eval()
        
        # ë©”íŠ¸ë¦­ ì´ˆê¸°í™”
        val_metrics = RegressionMetrics(self.device)
        epoch_loss = 0.0
        num_batches = 0
        
        # ì§„í–‰ ë°” ì„¤ì •
        pbar = tqdm(self.data_loader.val_loader, 
                   desc=f"Epoch {self.current_epoch+1} [Val]",
                   leave=False)
        
        with torch.no_grad():
            for batch_idx, (inputs, targets) in enumerate(pbar):
                # ë°ì´í„°ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                inputs = inputs.to(self.device)
                targets = targets.to(self.device)
                
                # ìˆœì „íŒŒ
                predictions = self.model(inputs)
                
                # ì†ì‹¤ ê³„ì‚°
                loss = self.loss_function(predictions, targets)
                
                # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                val_metrics.update(predictions, targets, loss)
                epoch_loss += loss.item()
                num_batches += 1
                
                # ì§„í–‰ ë°” ì—…ë°ì´íŠ¸
                pbar.set_postfix({
                    'Loss': f"{loss.item():.4f}",
                    'Avg Loss': f"{epoch_loss/num_batches:.4f}"
                })
        
        # ê²€ì¦ ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics = val_metrics.compute()
        metrics['avg_loss'] = epoch_loss / num_batches
        
        return metrics
    
    def test_model(self) -> Dict[str, float]:
        """
        ëª¨ë¸ í…ŒìŠ¤íŠ¸
        
        Returns:
            Dict[str, float]: í…ŒìŠ¤íŠ¸ ë©”íŠ¸ë¦­
        """
        self.model.eval()
        
        # ë©”íŠ¸ë¦­ ì´ˆê¸°í™”
        test_metrics = RegressionMetrics(self.device)
        
        print("ğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        with torch.no_grad():
            for inputs, targets in tqdm(self.data_loader.test_loader, desc="Testing"):
                # ë°ì´í„°ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                inputs = inputs.to(self.device)
                targets = targets.to(self.device)
                
                # ìˆœì „íŒŒ
                predictions = self.model(inputs)
                
                # ì†ì‹¤ ê³„ì‚°
                loss = self.loss_function(predictions, targets)
                
                # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                test_metrics.update(predictions, targets, loss)
        
        # í…ŒìŠ¤íŠ¸ ë©”íŠ¸ë¦­ ê³„ì‚° ë° ì¶œë ¥
        metrics = test_metrics.compute()
        
        print("ğŸ§ª í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        test_metrics.print_metrics()
        
        return metrics
    
    def save_checkpoint(self, 
                       metrics: Dict[str, Any], 
                       is_best: bool = False,
                       checkpoint_type: str = "epoch"):
        """
        ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        
        Args:
            metrics (Dict): í˜„ì¬ ë©”íŠ¸ë¦­
            is_best (bool): ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì—¬ë¶€
            checkpoint_type (str): ì²´í¬í¬ì¸íŠ¸ íƒ€ì…
        """
        # ì €ì¥í•  ì •ë³´ êµ¬ì„±
        checkpoint_info = {
            'train_metrics': metrics.get('train', {}),
            'val_metrics': metrics.get('val', {}),
            'best_val_mae': self.best_val_mae,
            'scheduler_state': self.scheduler.state_dict() if self.scheduler else None
        }
        
        # ëª¨ë¸ ì €ì¥
        if is_best:
            save_model_checkpoint(
                self.model, 
                str(self.best_model_path),
                epoch=self.current_epoch,
                loss=metrics.get('val', {}).get('avg_loss'),
                optimizer_state=self.optimizer.state_dict(),
                **checkpoint_info
            )
        
        # ë§ˆì§€ë§‰ ëª¨ë¸ ì €ì¥
        save_model_checkpoint(
            self.model,
            str(self.last_model_path),
            epoch=self.current_epoch,
            loss=metrics.get('val', {}).get('avg_loss'),
            optimizer_state=self.optimizer.state_dict(),
            **checkpoint_info
        )
    
    def train(self, 
             num_epochs: int,
             save_every: int = 5,
             validate_every: int = 1,
             early_stopping: bool = True,
             verbose: bool = True) -> Dict[str, Any]:
        """
        ëª¨ë¸ í›ˆë ¨
        
        Args:
            num_epochs (int): í›ˆë ¨ ì—í¬í¬ ìˆ˜
            save_every (int): ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì£¼ê¸°
            validate_every (int): ê²€ì¦ ìˆ˜í–‰ ì£¼ê¸°
            early_stopping (bool): ì¡°ê¸° ì¢…ë£Œ ì‚¬ìš© ì—¬ë¶€
            verbose (bool): ìƒì„¸ ì¶œë ¥ ì—¬ë¶€
            
        Returns:
            Dict[str, Any]: í›ˆë ¨ ê²°ê³¼
        """
        print(f"ğŸ‰ ìˆ˜ë°• ë‹¹ë„ ì˜ˆì¸¡ ëª¨ë¸ í›ˆë ¨ ì‹œì‘")
        print(f"   ğŸ“Š ì—í¬í¬: {num_epochs}")
        print(f"   ğŸ”„ ê²€ì¦ ì£¼ê¸°: {validate_every}")
        print(f"   ğŸ’¾ ì €ì¥ ì£¼ê¸°: {save_every}")
        print(f"   â¹ï¸ ì¡°ê¸° ì¢…ë£Œ: {early_stopping}")
        print("=" * 60)
        
        training_start_time = time.time()
        
        for epoch in range(num_epochs):
            self.current_epoch = epoch
            epoch_start_time = time.time()
            
            # í›ˆë ¨
            train_metrics = self.train_epoch()
            
            # ê²€ì¦
            if epoch % validate_every == 0:
                val_metrics = self.validate_epoch()
            else:
                val_metrics = {}
            
            # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§
            if self.scheduler:
                if isinstance(self.scheduler, ReduceLROnPlateau):
                    if val_metrics:
                        self.scheduler.step(val_metrics.get('mae', train_metrics['mae']))
                else:
                    self.scheduler.step()
            
            # ë©”íŠ¸ë¦­ ì¶”ì 
            if val_metrics:
                self.metrics_tracker.add_epoch_metrics(
                    train_metrics, 
                    val_metrics,
                    train_metrics['avg_loss']
                )
            
            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì²´í¬
            is_best = False
            if val_metrics and val_metrics.get('mae', float('inf')) < self.best_val_mae:
                self.best_val_mae = val_metrics['mae']
                is_best = True
                self.early_stopping_counter = 0
            elif val_metrics:
                self.early_stopping_counter += 1
            
            # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            if epoch % save_every == 0 or is_best or epoch == num_epochs - 1:
                metrics_dict = {'train': train_metrics, 'val': val_metrics}
                self.save_checkpoint(metrics_dict, is_best)
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            if verbose:
                epoch_time = time.time() - epoch_start_time
                print(f"\nğŸ“Š Epoch {epoch+1}/{num_epochs} ({epoch_time:.1f}s)")
                print(f"   ğŸš‚ Train - MAE: {train_metrics['mae']:.3f}, "
                      f"RMSE: {train_metrics['rmse']:.3f}, "
                      f"RÂ²: {train_metrics['r2_score']:.3f}")
                
                if val_metrics:
                    print(f"   ğŸ” Val   - MAE: {val_metrics['mae']:.3f}, "
                          f"RMSE: {val_metrics['rmse']:.3f}, "
                          f"RÂ²: {val_metrics['r2_score']:.3f}")
                    print(f"   {'ğŸ† BEST!' if is_best else ''}")
                
                # í•™ìŠµë¥  ì¶œë ¥
                current_lr = self.optimizer.param_groups[0]['lr']
                print(f"   ğŸšï¸ LR: {current_lr:.2e}")
            
            # ì¡°ê¸° ì¢…ë£Œ ì²´í¬
            if early_stopping and self.early_stopping_counter >= self.early_stopping_patience:
                print(f"\nâ¹ï¸ ì¡°ê¸° ì¢…ë£Œ: {self.early_stopping_patience} ì—í¬í¬ ë™ì•ˆ ê°œì„  ì—†ìŒ")
                break
        
        # í›ˆë ¨ ì™„ë£Œ
        total_training_time = time.time() - training_start_time
        print(f"\nâœ… í›ˆë ¨ ì™„ë£Œ!")
        print(f"   â±ï¸ ì´ ì‹œê°„: {total_training_time/3600:.2f}ì‹œê°„")
        print(f"   ğŸ† ìµœê³  Val MAE: {self.best_val_mae:.3f}")
        
        # ê²°ê³¼ ì €ì¥
        self._save_training_results()
        
        # ìµœì¢… í…ŒìŠ¤íŠ¸
        final_test_metrics = self.test_model()
        
        return {
            'best_val_mae': self.best_val_mae,
            'total_epochs': self.current_epoch + 1,
            'training_time': total_training_time,
            'final_test_metrics': final_test_metrics,
            'metrics_tracker': self.metrics_tracker
        }
    
    def _save_training_results(self):
        """í›ˆë ¨ ê²°ê³¼ ì €ì¥"""
        # ë©”íŠ¸ë¦­ ìš”ì•½ ì €ì¥
        metrics_path = self.save_dir / "metrics_summary.json"
        self.metrics_tracker.save_metrics_summary(str(metrics_path))
        
        # í›ˆë ¨ ê³¡ì„  ì €ì¥
        curves_path = self.save_dir / "training_curves.png"
        self.metrics_tracker.plot_training_curves(str(curves_path), show=False)
        
        print(f"ğŸ“Š í›ˆë ¨ ê²°ê³¼ ì €ì¥ë¨: {self.save_dir}")


def create_trainer_from_config(config_path: str, 
                              data_path: str,
                              experiment_name: Optional[str] = None) -> WatermelonTrainer:
    """
    ì„¤ì • íŒŒì¼ë¡œë¶€í„° íŠ¸ë ˆì´ë„ˆ ìƒì„±
    
    Args:
        config_path (str): í›ˆë ¨ ì„¤ì • íŒŒì¼ ê²½ë¡œ
        data_path (str): ë°ì´í„° ê²½ë¡œ
        experiment_name (str, optional): ì‹¤í—˜ ì´ë¦„
        
    Returns:
        WatermelonTrainer: ìƒì„±ëœ íŠ¸ë ˆì´ë„ˆ
    """
    # ì„¤ì • ë¡œë“œ
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸ–¥ï¸ ë””ë°”ì´ìŠ¤: {device}")
    
    # ëª¨ë¸ ìƒì„±
    from models.vgg_watermelon import create_vgg_watermelon
    model_config = config.get('model', {})
    model = create_vgg_watermelon(**model_config)
    model.print_model_info()
    
    # ë°ì´í„° ë¡œë” ìƒì„±
    from training.data_loader import create_data_loaders
    data_config = config.get('data', {})
    
    # create_data_loadersê°€ ì§€ì›í•˜ëŠ” íŒŒë¼ë¯¸í„°ë§Œ í•„í„°ë§
    supported_params = {
        'train_ratio', 'val_ratio', 'test_ratio', 'batch_size', 'num_workers', 
        'pin_memory', 'use_augmentation', 'stratify_by_sweetness', 'random_seed', 
        'split_file', 'config_path'
    }
    filtered_data_config = {k: v for k, v in data_config.items() if k in supported_params}
    
    data_loader = create_data_loaders(data_path, **filtered_data_config)
    
    # ì†ì‹¤ í•¨ìˆ˜ ìƒì„±
    loss_config = config.get('loss', {'type': 'mse'})
    loss_function = create_loss_function(loss_config)
    
    # ì˜µí‹°ë§ˆì´ì € ìƒì„±
    optimizer_config = config.get('optimizer', {})
    optimizer_type = optimizer_config.get('type', 'adam').lower()
    lr = float(optimizer_config.get('lr', 0.001))
    weight_decay = float(optimizer_config.get('weight_decay', 1e-4))
    
    if optimizer_type == 'adam':
        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    elif optimizer_type == 'sgd':
        optimizer = optim.SGD(
            model.parameters(), 
            lr=lr, 
            momentum=optimizer_config.get('momentum', 0.9),
            weight_decay=weight_decay
        )
    elif optimizer_type == 'adamw':
        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì˜µí‹°ë§ˆì´ì €: {optimizer_type}")
    
    # ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„±
    scheduler = None
    scheduler_config = config.get('scheduler', {})
    if scheduler_config.get('enabled', False):
        scheduler_type = scheduler_config.get('type', 'plateau').lower()
        
        if scheduler_type == 'plateau':
            scheduler = ReduceLROnPlateau(
                optimizer,
                mode='min',
                factor=scheduler_config.get('factor', 0.5),
                patience=scheduler_config.get('patience', 5)
            )
        elif scheduler_type == 'step':
            scheduler = StepLR(
                optimizer,
                step_size=scheduler_config.get('step_size', 10),
                gamma=scheduler_config.get('gamma', 0.1)
            )
        elif scheduler_type == 'cosine':
            scheduler = CosineAnnealingLR(
                optimizer,
                T_max=scheduler_config.get('T_max', 50),
                eta_min=scheduler_config.get('eta_min', 1e-6)
            )
    
    # ì‹¤í—˜ ì´ë¦„ ì„¤ì •
    if experiment_name is None:
        experiment_name = f"watermelon_{model_config.get('name', 'vgg')}"
    
    # íŠ¸ë ˆì´ë„ˆ ìƒì„±
    trainer = WatermelonTrainer(
        model=model,
        data_loader=data_loader,
        loss_function=loss_function,
        optimizer=optimizer,
        device=device,
        scheduler=scheduler,
        experiment_name=experiment_name
    )
    
    return trainer


if __name__ == "__main__":
    # íŠ¸ë ˆì´ë„ˆ í…ŒìŠ¤íŠ¸
    print("ğŸ§ª íŠ¸ë ˆì´ë„ˆ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸")
    
    # ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
    try:
        # ì„¤ì • íŒŒì¼ì´ ìˆë‹¤ë©´ ì‚¬ìš©
        config_path = "../../configs/training.yaml"
        data_path = "../../watermelon_sound_data"
        
        if Path(config_path).exists():
            trainer = create_trainer_from_config(config_path, data_path, "test_run")
            
            # ì§§ì€ í›ˆë ¨ ì‹¤í–‰
            results = trainer.train(num_epochs=2, verbose=True)
            
            print("âœ… íŠ¸ë ˆì´ë„ˆ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
            print(f"   ğŸ† ìµœê³  Val MAE: {results['best_val_mae']:.3f}")
        else:
            print(f"âš ï¸ ì„¤ì • íŒŒì¼ ì—†ìŒ: {config_path}")
            
    except Exception as e:
        print(f"âŒ íŠ¸ë ˆì´ë„ˆ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc() 