#!/usr/bin/env python3
"""
EfficientNet ëª¨ë¸ì„ Core MLë¡œ ë³€í™˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
PyTorch â†’ ONNX â†’ Core ML ê²½ë¡œ ì‹œë„ í›„, ì‹¤íŒ¨ì‹œ ì§ì ‘ ë³€í™˜
"""

import sys
import torch
import numpy as np
from pathlib import Path
import traceback

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent / 'src'))

from models.vgg_watermelon import VGGWatermelon


def load_model():
    """í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ"""
    model_path = "experiments/efficientnet_fixed_final/best_model.pth"
    
    print("ğŸ”„ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    
    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)
    
    # ëª¨ë¸ ì„¤ì • í™•ì¸
    model_config = checkpoint.get('model_config', {})
    print(f"ğŸ“‹ ëª¨ë¸ ì„¤ì •: {model_config}")
    
    # ëª¨ë¸ ìƒì„± (ì €ì¥ëœ ì„¤ì • ì‚¬ìš©)
    model = VGGWatermelon(
        fc_hidden_size=model_config.get('fc_hidden_size', 256),
        num_fc_layers=model_config.get('num_fc_layers', 2),
        dropout_rate=model_config.get('dropout_rate', 0.7)
    )
    
    if 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
    else:
        model.load_state_dict(checkpoint)
    
    model.eval()
    print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
    return model


def convert_to_onnx(model, output_path="models/converted/efficientnet_fixed.onnx"):
    """PyTorch ëª¨ë¸ì„ ONNXë¡œ ë³€í™˜"""
    try:
        print("ğŸ”„ ONNX ë³€í™˜ ì‹œì‘...")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ë”ë¯¸ ì…ë ¥ ìƒì„± (ë°°ì¹˜ í¬ê¸° 1, ì±„ë„ 3, 224x224)
        dummy_input = torch.randn(1, 3, 224, 224)
        
        # ONNX ë³€í™˜
        torch.onnx.export(
            model,
            (dummy_input,),
            output_path,
            export_params=True,
            opset_version=11,
            do_constant_folding=True,
            input_names=['input'],
            output_names=['output'],
            dynamic_axes={
                'input': {0: 'batch_size'},
                'output': {0: 'batch_size'}
            }
        )
        
        print(f"âœ… ONNX ë³€í™˜ ì™„ë£Œ: {output_path}")
        return output_path
        
    except Exception as e:
        print(f"âŒ ONNX ë³€í™˜ ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        return None


def convert_onnx_to_coreml(onnx_path, output_path="models/converted/efficientnet_fixed.mlmodel"):
    """ONNX ëª¨ë¸ì„ Core MLë¡œ ë³€í™˜"""
    try:
        import coremltools as ct
        print("ğŸ”„ Core ML ë³€í™˜ ì‹œì‘...")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ONNX ëª¨ë¸ ë¡œë“œ ë° Core ML ë³€í™˜
        model = ct.convert(
            str(onnx_path),
            inputs=[ct.TensorType(shape=(1, 3, 224, 224), name="input")],
            outputs=[ct.TensorType(name="output")],
            compute_units=ct.ComputeUnit.CPU_AND_GPU,
            minimum_deployment_target=ct.target.iOS13
        )
        
        # ëª¨ë¸ ë©”íƒ€ë°ì´í„° ì„¤ì •
        model.short_description = "ìˆ˜ë°• ë‹¹ë„ ì˜ˆì¸¡ ëª¨ë¸ (EfficientNet ê¸°ë°˜)"
        model.author = "Watermelon ML Team"
        model.license = "MIT"
        model.version = "1.0"
        
        # ì…ë ¥/ì¶œë ¥ ì„¤ëª… ì¶”ê°€
        model.input_description["input"] = "ë©œ-ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ì´ë¯¸ì§€ (224x224x3)"
        model.output_description["output"] = "ì˜ˆì¸¡ëœ ë‹¹ë„ê°’ (Brix)"
        
        # ëª¨ë¸ ì €ì¥
        model.save(str(output_path))
        
        print(f"âœ… Core ML ë³€í™˜ ì™„ë£Œ: {output_path}")
        return output_path
        
    except Exception as e:
        print(f"âŒ Core ML ë³€í™˜ ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        return None


def convert_pytorch_to_coreml_direct(model, output_path="models/converted/efficientnet_fixed_direct.mlmodel"):
    """PyTorch ëª¨ë¸ì„ Core MLë¡œ ì§ì ‘ ë³€í™˜"""
    try:
        import coremltools as ct
        print("ğŸ”„ PyTorch â†’ Core ML ì§ì ‘ ë³€í™˜ ì‹œì‘...")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ë”ë¯¸ ì…ë ¥ ìƒì„±
        dummy_input = torch.randn(1, 3, 224, 224)
        
        # PyTorch ëª¨ë¸ì„ traced modelë¡œ ë³€í™˜
        traced_model = torch.jit.trace(model, dummy_input)
        
        # Core ML ë³€í™˜
        coreml_model = ct.convert(
            traced_model,
            inputs=[ct.TensorType(shape=(1, 3, 224, 224), name="input")],
            outputs=[ct.TensorType(name="output")],
            compute_units=ct.ComputeUnit.CPU_AND_GPU,
            minimum_deployment_target=ct.target.iOS13
        )
        
        # ëª¨ë¸ ë©”íƒ€ë°ì´í„° ì„¤ì •
        coreml_model.short_description = "ìˆ˜ë°• ë‹¹ë„ ì˜ˆì¸¡ ëª¨ë¸ (EfficientNet ê¸°ë°˜ - ì§ì ‘ ë³€í™˜)"
        coreml_model.author = "Watermelon ML Team"
        coreml_model.license = "MIT"
        coreml_model.version = "1.0"
        
        # ì…ë ¥/ì¶œë ¥ ì„¤ëª… ì¶”ê°€
        coreml_model.input_description["input"] = "ë©œ-ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ì´ë¯¸ì§€ (224x224x3)"
        coreml_model.output_description["output"] = "ì˜ˆì¸¡ëœ ë‹¹ë„ê°’ (Brix)"
        
        # ëª¨ë¸ ì €ì¥
        coreml_model.save(str(output_path))
        
        print(f"âœ… Core ML ì§ì ‘ ë³€í™˜ ì™„ë£Œ: {output_path}")
        return output_path
        
    except Exception as e:
        print(f"âŒ Core ML ì§ì ‘ ë³€í™˜ ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        return None


def test_converted_model(model_path, original_model):
    """ë³€í™˜ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    try:
        import coremltools as ct
        print(f"ğŸ§ª ë³€í™˜ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸: {model_path}")
        
        # Core ML ëª¨ë¸ ë¡œë“œ
        coreml_model = ct.models.MLModel(str(model_path))
        
        # ë”ë¯¸ ì…ë ¥ ìƒì„±
        dummy_input = torch.randn(1, 3, 224, 224)
        
        # ì›ë³¸ PyTorch ëª¨ë¸ ì˜ˆì¸¡
        with torch.no_grad():
            original_output = original_model(dummy_input)
            original_pred = original_output.numpy().flatten()[0]
        
        # Core ML ëª¨ë¸ ì˜ˆì¸¡
        input_dict = {"input": dummy_input.numpy()}
        coreml_output = coreml_model.predict(input_dict)
        coreml_pred = list(coreml_output.values())[0][0]
        
        # ê²°ê³¼ ë¹„êµ
        diff = abs(original_pred - coreml_pred)
        print(f"   ğŸ“Š ì›ë³¸ ëª¨ë¸ ì˜ˆì¸¡: {original_pred:.6f}")
        print(f"   ğŸ“Š Core ML ì˜ˆì¸¡: {coreml_pred:.6f}")
        print(f"   ğŸ“Š ì°¨ì´: {diff:.6f}")
        
        if diff < 0.001:
            print("   âœ… ë³€í™˜ ì„±ê³µ! ì˜ˆì¸¡ê°’ì´ ì¼ì¹˜í•©ë‹ˆë‹¤.")
        else:
            print("   âš ï¸ ë³€í™˜ëœ ëª¨ë¸ê³¼ ì›ë³¸ ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ì— ì°¨ì´ê°€ ìˆìŠµë‹ˆë‹¤.")
        
        return True
        
    except Exception as e:
        print(f"   âŒ ë³€í™˜ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False


def main():
    """ë©”ì¸ ë³€í™˜ í•¨ìˆ˜"""
    print("ğŸ‰ EfficientNet ëª¨ë¸ â†’ Core ML ë³€í™˜ ì‹œì‘!")
    print("="*60)
    
    # 1. ëª¨ë¸ ë¡œë“œ
    model = load_model()
    
    # 2. PyTorch â†’ ONNX â†’ Core ML ì‹œë„
    print("\nğŸ“ ë°©ë²• 1: PyTorch â†’ ONNX â†’ Core ML")
    onnx_path = convert_to_onnx(model)
    
    if onnx_path:
        coreml_path = convert_onnx_to_coreml(onnx_path)
        if coreml_path:
            # ë³€í™˜ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸
            if test_converted_model(coreml_path, model):
                print(f"\nğŸ‰ ë³€í™˜ ì™„ë£Œ! Core ML ëª¨ë¸: {coreml_path}")
                return coreml_path
    
    # 3. PyTorch â†’ Core ML ì§ì ‘ ë³€í™˜ ì‹œë„
    print("\nğŸ“ ë°©ë²• 2: PyTorch â†’ Core ML ì§ì ‘ ë³€í™˜")
    coreml_path_direct = convert_pytorch_to_coreml_direct(model)
    
    if coreml_path_direct:
        # ë³€í™˜ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸
        if test_converted_model(coreml_path_direct, model):
            print(f"\nğŸ‰ ì§ì ‘ ë³€í™˜ ì™„ë£Œ! Core ML ëª¨ë¸: {coreml_path_direct}")
            return coreml_path_direct
    
    print("\nâŒ ëª¨ë“  ë³€í™˜ ë°©ë²•ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    return None


if __name__ == "__main__":
    result = main()
    if result:
        print(f"\nâœ… ìµœì¢… ê²°ê³¼: {result}")
        print("ğŸ“± iOS ì•±ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")
    else:
        print("\nâŒ ë³€í™˜ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.") 