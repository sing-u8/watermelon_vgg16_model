"""
Metrics Module
ìˆ˜ë°• ë‹¹ë„ ì˜ˆì¸¡ ëª¨ë¸ í‰ê°€ë¥¼ ìœ„í•œ ë©”íŠ¸ë¦­ ëª¨ë“ˆ
"""

import torch
import numpy as np
from typing import Dict, List, Optional, Tuple
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
import matplotlib.pyplot as plt
from pathlib import Path


class RegressionMetrics:
    """
    íšŒê·€ ëª¨ë¸ í‰ê°€ë¥¼ ìœ„í•œ ë©”íŠ¸ë¦­ í´ë˜ìŠ¤
    """
    
    def __init__(self, device: Optional[torch.device] = None):
        """
        RegressionMetrics ì´ˆê¸°í™”
        
        Args:
            device (torch.device, optional): ê³„ì‚°ì— ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤
        """
        self.device = device or torch.device('cpu')
        self.reset()
    
    def reset(self):
        """ë©”íŠ¸ë¦­ ì´ˆê¸°í™”"""
        self.predictions = []
        self.targets = []
        self.losses = []
    
    def update(self, 
               predictions: torch.Tensor, 
               targets: torch.Tensor, 
               loss: Optional[torch.Tensor] = None):
        """
        ë°°ì¹˜ ê²°ê³¼ë¡œ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        
        Args:
            predictions (torch.Tensor): ëª¨ë¸ ì˜ˆì¸¡ê°’
            targets (torch.Tensor): ì‹¤ì œ íƒ€ê²Ÿê°’
            loss (torch.Tensor, optional): ì†ì‹¤ê°’
        """
        # CPUë¡œ ì´ë™ ë° numpy ë³€í™˜
        pred_np = predictions.detach().cpu().numpy().flatten()
        target_np = targets.detach().cpu().numpy().flatten()
        
        self.predictions.extend(pred_np)
        self.targets.extend(target_np)
        
        if loss is not None:
            loss_val = loss.detach().cpu().item()
            self.losses.append(loss_val)
    
    def compute(self) -> Dict[str, float]:
        """
        ëª¨ë“  ë©”íŠ¸ë¦­ ê³„ì‚°
        
        Returns:
            Dict[str, float]: ê³„ì‚°ëœ ë©”íŠ¸ë¦­ë“¤
        """
        if len(self.predictions) == 0:
            return {}
        
        predictions = np.array(self.predictions)
        targets = np.array(self.targets)
        
        # ê¸°ë³¸ íšŒê·€ ë©”íŠ¸ë¦­
        mae = mean_absolute_error(targets, predictions)
        mse = mean_squared_error(targets, predictions)
        rmse = np.sqrt(mse)
        r2 = r2_score(targets, predictions)
        
        # ì¶”ê°€ ë©”íŠ¸ë¦­
        mape = self._calculate_mape(targets, predictions)
        pearson_corr = self._calculate_pearson_correlation(targets, predictions)
        sweetness_accuracy = self._calculate_sweetness_accuracy(targets, predictions)
        
        # ì˜¤ì°¨ í†µê³„
        errors = predictions - targets
        mean_error = np.mean(errors)
        std_error = np.std(errors)
        max_error = np.max(np.abs(errors))
        
        metrics = {
            'mae': mae,
            'mse': mse,
            'rmse': rmse,
            'r2_score': r2,
            'mape': mape,
            'pearson_correlation': pearson_corr,
            'sweetness_accuracy_05': sweetness_accuracy[0],  # Â±0.5 Brix
            'sweetness_accuracy_10': sweetness_accuracy[1],  # Â±1.0 Brix
            'mean_error': mean_error,
            'std_error': std_error,
            'max_error': max_error,
            'num_samples': len(predictions)
        }
        
        # ì†ì‹¤ í‰ê·  ì¶”ê°€
        if self.losses:
            metrics['avg_loss'] = np.mean(self.losses)
        
        return metrics
    
    def _calculate_mape(self, targets: np.ndarray, predictions: np.ndarray) -> float:
        """í‰ê·  ì ˆëŒ€ ë°±ë¶„ìœ¨ ì˜¤ì°¨ (MAPE) ê³„ì‚°"""
        # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€
        epsilon = 1e-8
        return float(np.mean(np.abs((targets - predictions) / (targets + epsilon))) * 100)
    
    def _calculate_pearson_correlation(self, targets: np.ndarray, predictions: np.ndarray) -> float:
        """í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ ê³„ì‚°"""
        correlation_matrix = np.corrcoef(targets, predictions)
        return correlation_matrix[0, 1] if not np.isnan(correlation_matrix[0, 1]) else 0.0
    
    def _calculate_sweetness_accuracy(self, targets: np.ndarray, predictions: np.ndarray) -> Tuple[float, float]:
        """ë‹¹ë„ ì˜ˆì¸¡ ì •í™•ë„ ê³„ì‚° (Â±0.5, Â±1.0 Brix ë‚´ ì •í™•ë„)"""
        errors = np.abs(targets - predictions)
        
        accuracy_05 = float(np.mean(errors <= 0.5) * 100)  # Â±0.5 Brix ë‚´ ì •í™•ë„
        accuracy_10 = float(np.mean(errors <= 1.0) * 100)  # Â±1.0 Brix ë‚´ ì •í™•ë„
        
        return accuracy_05, accuracy_10
    
    def print_metrics(self):
        """ë©”íŠ¸ë¦­ ì¶œë ¥"""
        metrics = self.compute()
        
        if not metrics:
            print("ğŸ“Š ë©”íŠ¸ë¦­: ë°ì´í„° ì—†ìŒ")
            return
        
        print(f"ğŸ“Š íšŒê·€ ë©”íŠ¸ë¦­ ê²°ê³¼ (ìƒ˜í”Œ ìˆ˜: {metrics['num_samples']:,})")
        print(f"   ğŸ“ MAE: {metrics['mae']:.3f}")
        print(f"   ğŸ“ MSE: {metrics['mse']:.3f}")
        print(f"   ğŸ“Š RMSE: {metrics['rmse']:.3f}")
        print(f"   ğŸ¯ RÂ² Score: {metrics['r2_score']:.3f}")
        print(f"   ğŸ“ˆ MAPE: {metrics['mape']:.2f}%")
        print(f"   ğŸ”— Pearson ìƒê´€ê³„ìˆ˜: {metrics['pearson_correlation']:.3f}")
        print(f"   âœ… ë‹¹ë„ ì •í™•ë„ (Â±0.5): {metrics['sweetness_accuracy_05']:.1f}%")
        print(f"   âœ… ë‹¹ë„ ì •í™•ë„ (Â±1.0): {metrics['sweetness_accuracy_10']:.1f}%")
        print(f"   ğŸ“Š í‰ê·  ì˜¤ì°¨: {metrics['mean_error']:.3f} Â± {metrics['std_error']:.3f}")
        print(f"   ğŸ”´ ìµœëŒ€ ì˜¤ì°¨: {metrics['max_error']:.3f}")
        
        if 'avg_loss' in metrics:
            print(f"   ğŸ’” í‰ê·  ì†ì‹¤: {metrics['avg_loss']:.4f}")


class MetricsTracker:
    """
    í›ˆë ¨ ê³¼ì •ì—ì„œ ë©”íŠ¸ë¦­ì„ ì¶”ì í•˜ëŠ” í´ë˜ìŠ¤
    """
    
    def __init__(self):
        """MetricsTracker ì´ˆê¸°í™”"""
        self.train_metrics = []
        self.val_metrics = []
        self.epoch_losses = []
    
    def add_epoch_metrics(self, 
                         train_metrics: Dict[str, float], 
                         val_metrics: Dict[str, float],
                         epoch_loss: Optional[float] = None):
        """
        ì—í¬í¬ ë©”íŠ¸ë¦­ ì¶”ê°€
        
        Args:
            train_metrics (Dict): í›ˆë ¨ ë©”íŠ¸ë¦­
            val_metrics (Dict): ê²€ì¦ ë©”íŠ¸ë¦­
            epoch_loss (float, optional): ì—í¬í¬ ì†ì‹¤
        """
        self.train_metrics.append(train_metrics)
        self.val_metrics.append(val_metrics)
        
        if epoch_loss is not None:
            self.epoch_losses.append(epoch_loss)
    
    def get_best_epoch(self, metric_name: str = 'val_mae', minimize: bool = True) -> int:
        """
        ìµœì  ì—í¬í¬ ì°¾ê¸°
        
        Args:
            metric_name (str): ê¸°ì¤€ ë©”íŠ¸ë¦­ ì´ë¦„
            minimize (bool): ìµœì†Œí™” ì—¬ë¶€ (True: ìµœì†Œê°’, False: ìµœëŒ€ê°’)
            
        Returns:
            int: ìµœì  ì—í¬í¬ ë²ˆí˜¸
        """
        if not self.val_metrics:
            return -1
        
        # ê²€ì¦ ë©”íŠ¸ë¦­ì—ì„œ í•´ë‹¹ ë©”íŠ¸ë¦­ ì¶”ì¶œ
        metric_key = metric_name.replace('val_', '')
        values = [metrics.get(metric_key, float('inf') if minimize else float('-inf')) 
                 for metrics in self.val_metrics]
        
        if minimize:
            return int(np.argmin(values))
        else:
            return int(np.argmax(values))
    
    def plot_training_curves(self, 
                           save_path: Optional[str] = None,
                           show: bool = True):
        """
        í›ˆë ¨ ê³¡ì„  í”Œë¡¯
        
        Args:
            save_path (str, optional): ì €ì¥ ê²½ë¡œ
            show (bool): í”Œë¡¯ í‘œì‹œ ì—¬ë¶€
        """
        if not self.train_metrics or not self.val_metrics:
            print("âš ï¸ í”Œë¡¯í•  ë©”íŠ¸ë¦­ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('ğŸ‰ Watermelon Sweetness Prediction - Training Curves', fontsize=16)
        
        epochs = range(1, len(self.train_metrics) + 1)
        
        # 1. MAE ê³¡ì„ 
        train_mae = [m.get('mae', 0) for m in self.train_metrics]
        val_mae = [m.get('mae', 0) for m in self.val_metrics]
        
        axes[0, 0].plot(epochs, train_mae, 'b-', label='Train MAE', linewidth=2)
        axes[0, 0].plot(epochs, val_mae, 'r-', label='Val MAE', linewidth=2)
        axes[0, 0].set_title('Mean Absolute Error (MAE)')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('MAE')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # 2. RÂ² Score ê³¡ì„ 
        train_r2 = [m.get('r2_score', 0) for m in self.train_metrics]
        val_r2 = [m.get('r2_score', 0) for m in self.val_metrics]
        
        axes[0, 1].plot(epochs, train_r2, 'b-', label='Train RÂ²', linewidth=2)
        axes[0, 1].plot(epochs, val_r2, 'r-', label='Val RÂ²', linewidth=2)
        axes[0, 1].set_title('RÂ² Score')
        axes[0, 1].set_xlabel('Epoch')
        axes[0, 1].set_ylabel('RÂ² Score')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # 3. RMSE ê³¡ì„ 
        train_rmse = [m.get('rmse', 0) for m in self.train_metrics]
        val_rmse = [m.get('rmse', 0) for m in self.val_metrics]
        
        axes[1, 0].plot(epochs, train_rmse, 'b-', label='Train RMSE', linewidth=2)
        axes[1, 0].plot(epochs, val_rmse, 'r-', label='Val RMSE', linewidth=2)
        axes[1, 0].set_title('Root Mean Square Error (RMSE)')
        axes[1, 0].set_xlabel('Epoch')
        axes[1, 0].set_ylabel('RMSE')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # 4. ë‹¹ë„ ì •í™•ë„ ê³¡ì„ 
        train_acc = [m.get('sweetness_accuracy_05', 0) for m in self.train_metrics]
        val_acc = [m.get('sweetness_accuracy_05', 0) for m in self.val_metrics]
        
        axes[1, 1].plot(epochs, train_acc, 'b-', label='Train Acc (Â±0.5)', linewidth=2)
        axes[1, 1].plot(epochs, val_acc, 'r-', label='Val Acc (Â±0.5)', linewidth=2)
        axes[1, 1].set_title('Sweetness Accuracy (Â±0.5 Brix)')
        axes[1, 1].set_xlabel('Epoch')
        axes[1, 1].set_ylabel('Accuracy (%)')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ì €ì¥
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š í›ˆë ¨ ê³¡ì„  ì €ì¥ë¨: {save_path}")
        
        # í‘œì‹œ
        if show:
            plt.show()
        else:
            plt.close()
    
    def plot_prediction_scatter(self, 
                              predictions: np.ndarray,
                              targets: np.ndarray,
                              title: str = "Prediction vs Target",
                              save_path: Optional[str] = None,
                              show: bool = True):
        """
        ì˜ˆì¸¡ê°’ vs ì‹¤ì œê°’ ì‚°ì ë„ í”Œë¡¯
        
        Args:
            predictions (np.ndarray): ì˜ˆì¸¡ê°’
            targets (np.ndarray): ì‹¤ì œê°’
            title (str): í”Œë¡¯ ì œëª©
            save_path (str, optional): ì €ì¥ ê²½ë¡œ
            show (bool): í”Œë¡¯ í‘œì‹œ ì—¬ë¶€
        """
        plt.figure(figsize=(10, 8))
        
        # ì‚°ì ë„
        plt.scatter(targets, predictions, alpha=0.6, s=50, edgecolors='black', linewidth=0.5)
        
        # ì™„ë²½í•œ ì˜ˆì¸¡ ë¼ì¸ (y=x)
        min_val = min(targets.min(), predictions.min())
        max_val = max(targets.max(), predictions.max())
        plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')
        
        # Â±0.5, Â±1.0 Brix í—ˆìš© ì˜¤ì°¨ ì˜ì—­
        plt.fill_between([min_val, max_val], [min_val-0.5, max_val-0.5], 
                        [min_val+0.5, max_val+0.5], alpha=0.2, color='green', 
                        label='Â±0.5 Brix tolerance')
        plt.fill_between([min_val, max_val], [min_val-1.0, max_val-1.0], 
                        [min_val+1.0, max_val+1.0], alpha=0.1, color='blue', 
                        label='Â±1.0 Brix tolerance')
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        mae = mean_absolute_error(targets, predictions)
        r2 = r2_score(targets, predictions)
        
        plt.xlabel('Actual Sweetness (Brix)', fontsize=12)
        plt.ylabel('Predicted Sweetness (Brix)', fontsize=12)
        plt.title(f'{title}\nMAE: {mae:.3f}, RÂ²: {r2:.3f}', fontsize=14)
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # ì¶• ë²”ìœ„ ì„¤ì •
        plt.xlim(min_val - 0.5, max_val + 0.5)
        plt.ylim(min_val - 0.5, max_val + 0.5)
        
        # ì €ì¥
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š ì‚°ì ë„ ì €ì¥ë¨: {save_path}")
        
        # í‘œì‹œ
        if show:
            plt.show()
        else:
            plt.close()
    
    def _convert_numpy_to_python(self, obj):
        """
        NumPy íƒ€ì…ì„ Python ê¸°ë³¸ íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
        
        Args:
            obj: ë³€í™˜í•  ê°ì²´
            
        Returns:
            ë³€í™˜ëœ ê°ì²´ (JSON ì§ë ¬í™” ê°€ëŠ¥)
        """
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {key: self._convert_numpy_to_python(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_numpy_to_python(item) for item in obj]
        else:
            return obj
    
    def save_metrics_summary(self, save_path: str):
        """
        ë©”íŠ¸ë¦­ ìš”ì•½ ì €ì¥
        
        Args:
            save_path (str): ì €ì¥ ê²½ë¡œ
        """
        if not self.train_metrics or not self.val_metrics:
            print("âš ï¸ ì €ì¥í•  ë©”íŠ¸ë¦­ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ìµœì¢… ë©”íŠ¸ë¦­ ì¶”ì¶œ
        final_train = self.train_metrics[-1]
        final_val = self.val_metrics[-1]
        
        # ìµœì  ì—í¬í¬ ì°¾ê¸°
        best_mae_epoch = self.get_best_epoch('mae', minimize=True)
        best_r2_epoch = self.get_best_epoch('r2_score', minimize=False)
        
        summary = {
            'training_summary': {
                'total_epochs': len(self.train_metrics),
                'best_mae_epoch': best_mae_epoch + 1,
                'best_r2_epoch': best_r2_epoch + 1
            },
            'final_metrics': {
                'train': final_train,
                'validation': final_val
            },
            'best_metrics': {
                'best_mae': {
                    'epoch': best_mae_epoch + 1,
                    'train': self.train_metrics[best_mae_epoch],
                    'val': self.val_metrics[best_mae_epoch]
                },
                'best_r2': {
                    'epoch': best_r2_epoch + 1,
                    'train': self.train_metrics[best_r2_epoch],
                    'val': self.val_metrics[best_r2_epoch]
                }
            }
        }
        
        # NumPy íƒ€ì…ì„ Python íƒ€ì…ìœ¼ë¡œ ë³€í™˜
        summary = self._convert_numpy_to_python(summary)
        
        # JSONìœ¼ë¡œ ì €ì¥
        import json
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“Š ë©”íŠ¸ë¦­ ìš”ì•½ ì €ì¥ë¨: {save_path}")


if __name__ == "__main__":
    # ë©”íŠ¸ë¦­ í…ŒìŠ¤íŠ¸
    print("ğŸ§ª ë©”íŠ¸ë¦­ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    np.random.seed(42)
    targets = np.random.uniform(8.0, 13.0, 100)  # ì‹¤ì œ ë‹¹ë„ê°’
    noise = np.random.normal(0, 0.5, 100)  # ë…¸ì´ì¦ˆ
    predictions = targets + noise  # ë…¸ì´ì¦ˆê°€ ìˆëŠ” ì˜ˆì¸¡ê°’
    
    # ë©”íŠ¸ë¦­ ê³„ì‚°
    metrics = RegressionMetrics()
    
    # ë°°ì¹˜ë³„ë¡œ ì—…ë°ì´íŠ¸ ì‹œë®¬ë ˆì´ì…˜
    batch_size = 10
    for i in range(0, len(targets), batch_size):
        batch_targets = torch.tensor(targets[i:i+batch_size], dtype=torch.float32)
        batch_predictions = torch.tensor(predictions[i:i+batch_size], dtype=torch.float32)
        
        metrics.update(batch_predictions, batch_targets)
    
    # ê²°ê³¼ ì¶œë ¥
    metrics.print_metrics()
    
    # ë©”íŠ¸ë¦­ ì¶”ì ê¸° í…ŒìŠ¤íŠ¸
    tracker = MetricsTracker()
    
    # ê°€ìƒì˜ í›ˆë ¨ ê³¼ì • ì‹œë®¬ë ˆì´ì…˜
    for epoch in range(5):
        train_m = {
            'mae': 1.0 - epoch * 0.1,
            'r2_score': 0.5 + epoch * 0.08,
            'rmse': 1.5 - epoch * 0.15
        }
        val_m = {
            'mae': 1.2 - epoch * 0.08,
            'r2_score': 0.4 + epoch * 0.09,
            'rmse': 1.8 - epoch * 0.12
        }
        tracker.add_epoch_metrics(train_m, val_m)
    
    print(f"\nğŸ¯ ìµœì  ì—í¬í¬ (MAE ê¸°ì¤€): {tracker.get_best_epoch('mae') + 1}")
    print(f"ğŸ¯ ìµœì  ì—í¬í¬ (RÂ² ê¸°ì¤€): {tracker.get_best_epoch('r2_score', minimize=False) + 1}")
    
    print("âœ… ë©”íŠ¸ë¦­ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì™„ë£Œ") 