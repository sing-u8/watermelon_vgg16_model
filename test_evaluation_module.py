#!/usr/bin/env python3
"""
í‰ê°€ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
Phase 3.6: í‰ê°€ ëª¨ë“ˆ ì™„ì„± ë° ê²€ì¦
"""

import numpy as np
import torch
import sys
from pathlib import Path

def test_evaluation_module():
    """í‰ê°€ ëª¨ë“ˆ ì „ì²´ í…ŒìŠ¤íŠ¸"""
    
    print("ğŸ§ª í‰ê°€ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 50)
    
    try:
        # 1. ê¸°ë³¸ ëª¨ë“ˆ import í…ŒìŠ¤íŠ¸
        print("ğŸ“¦ ëª¨ë“ˆ import í…ŒìŠ¤íŠ¸...")
        
        from src.evaluation.evaluator import WatermelonEvaluator, create_evaluator, quick_evaluation
        print("   âœ… evaluator ëª¨ë“ˆ import ì„±ê³µ")
        
        from src.evaluation.model_analyzer import ModelAnalyzer
        print("   âœ… model_analyzer ëª¨ë“ˆ import ì„±ê³µ")
        
        from src.evaluation.visualization import create_evaluation_plots
        print("   âœ… visualization ëª¨ë“ˆ import ì„±ê³µ")
        
        from src.evaluation import WatermelonEvaluator, ModelAnalyzer, create_evaluation_plots
        print("   âœ… í†µí•© import ì„±ê³µ")
        
        # 2. ë”ë¯¸ ëª¨ë¸ ìƒì„± í…ŒìŠ¤íŠ¸
        print("\nğŸ¤– ë”ë¯¸ ëª¨ë¸ ìƒì„± í…ŒìŠ¤íŠ¸...")
        
        from src.models.vgg_watermelon import create_vgg_watermelon
        model = create_vgg_watermelon()
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"   âœ… VGG ëª¨ë¸ ìƒì„± ì„±ê³µ (device: {device})")
        
        # 3. í‰ê°€ê¸° ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
        print("\nğŸ” í‰ê°€ê¸° ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸...")
        
        evaluator = WatermelonEvaluator(model, device, "test_evaluation_results")
        print("   âœ… WatermelonEvaluator ì´ˆê¸°í™” ì„±ê³µ")
        
        # 4. ëª¨ë¸ ë¶„ì„ê¸° ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
        print("\nğŸ”¬ ëª¨ë¸ ë¶„ì„ê¸° ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸...")
        
        analyzer = ModelAnalyzer(model, device)
        print("   âœ… ModelAnalyzer ì´ˆê¸°í™” ì„±ê³µ")
        
        # 5. ì•„í‚¤í…ì²˜ ë¶„ì„ í…ŒìŠ¤íŠ¸
        print("\nğŸ—ï¸ ì•„í‚¤í…ì²˜ ë¶„ì„ í…ŒìŠ¤íŠ¸...")
        
        arch_analysis = analyzer.analyze_model_architecture()
        print(f"   âœ… ì•„í‚¤í…ì²˜ ë¶„ì„ ì™„ë£Œ (ì´ íŒŒë¼ë¯¸í„°: {arch_analysis['model_info']['total_parameters']:,})")
        
        # 6. ë”ë¯¸ ë°ì´í„°ë¡œ ì‹œê°í™” í…ŒìŠ¤íŠ¸
        print("\nğŸ“Š ì‹œê°í™” í…ŒìŠ¤íŠ¸...")
        
        # ë”ë¯¸ í‰ê°€ ê²°ê³¼ ìƒì„±
        np.random.seed(42)
        n_samples = 50
        targets = np.random.uniform(8.5, 12.5, n_samples)
        predictions = targets + np.random.normal(0, 0.3, n_samples)
        
        dummy_results = {
            'predictions': predictions.tolist(),
            'targets': targets.tolist(),
            'metrics': {
                'mae': 0.25,
                'rmse': 0.35,
                'r2_score': 0.85,
                'mape': 2.5,
                'sweetness_accuracy_05': 75.0,
                'sweetness_accuracy_10': 90.0,
                'pearson_correlation': 0.92,
                'num_samples': n_samples
            },
            'performance_analysis': {
                'range_analysis': {
                    'low_sweetness': {
                        'count': 15,
                        'mae': 0.22,
                        'rmse': 0.31,
                        'r2_score': 0.88,
                        'range': (8.0, 10.0)
                    },
                    'medium_sweetness': {
                        'count': 20,
                        'mae': 0.25,
                        'rmse': 0.35,
                        'r2_score': 0.85,
                        'range': (10.0, 11.5)
                    },
                    'high_sweetness': {
                        'count': 15,
                        'mae': 0.28,
                        'rmse': 0.38,
                        'r2_score': 0.82,
                        'range': (11.5, 13.0)
                    }
                }
            }
        }
        
        # ì‹œê°í™” ìƒì„±
        plot_paths = create_evaluation_plots(dummy_results, "test_evaluation_plots", show_plots=False)
        print(f"   âœ… {len(plot_paths)}ê°œ ì‹œê°í™” í”Œë¡¯ ìƒì„± ì„±ê³µ")
        
        # 7. í‰ê°€ ë³´ê³ ì„œ ìƒì„± í…ŒìŠ¤íŠ¸
        print("\nğŸ“„ í‰ê°€ ë³´ê³ ì„œ ìƒì„± í…ŒìŠ¤íŠ¸...")
        
        report_path = evaluator.generate_evaluation_report(dummy_results, include_plots=False)
        print(f"   âœ… í‰ê°€ ë³´ê³ ì„œ ìƒì„± ì„±ê³µ: {report_path}")
        
        print("\nğŸ‰ ëª¨ë“  í‰ê°€ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        print("âœ… Phase 3.6: í‰ê°€ ëª¨ë“ˆ ì™„ì„± ë° ê²€ì¦ - ì™„ë£Œ")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ í™•ì¸
    current_dir = Path.cwd()
    print(f"ğŸ“ í˜„ì¬ ë””ë ‰í† ë¦¬: {current_dir}")
    
    # Python ê²½ë¡œì— í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
    if str(current_dir) not in sys.path:
        sys.path.insert(0, str(current_dir))
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    success = test_evaluation_module()
    
    if success:
        print("\nğŸ“‹ ë‹¤ìŒ ë‹¨ê³„:")
        print("   - Phase 4: ëª¨ë¸ í›ˆë ¨ ì‹¤í–‰ ë° ì‹¤í—˜")
        print("   - ì‹¤ì œ ë°ì´í„°ì…‹ìœ¼ë¡œ ëª¨ë¸ í›ˆë ¨")
        print("   - í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹")
        print("   - ìµœì  ëª¨ë¸ ì„ íƒ")
        
        # Todo ì—…ë°ì´íŠ¸
        try:
            print("\nğŸ“ ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ ì¤‘...")
            # ì—¬ê¸°ì„œ todo ì—…ë°ì´íŠ¸ ë¡œì§ì„ ì¶”ê°€í•  ìˆ˜ ìˆìŒ
            print("   âœ… Phase 3.6 ì™„ë£Œ ë§ˆí‚¹")
        except:
            pass
    
    return success

if __name__ == "__main__":
    main() 