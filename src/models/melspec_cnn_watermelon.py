"""
Mel-Spectrogram CNN ê¸°ë°˜ ìˆ˜ë°• ë‹¹ë„ ì˜ˆì¸¡ ëª¨ë¸
ì˜¤ë””ì˜¤ ë„ë©”ì¸ì— íŠ¹í™”ëœ ê²½ëŸ‰ CNN ëª¨ë¸
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Dict, Any, Optional
import yaml
from pathlib import Path


class MelSpecCNNWatermelon(nn.Module):
    """
    Mel-Spectrogram CNN ê¸°ë°˜ ìˆ˜ë°• ë‹¹ë„ ì˜ˆì¸¡ ëª¨ë¸
    
    ì˜¤ë””ì˜¤ ë©œ-ìŠ¤í™íŠ¸ë¡œê·¸ë¨ ì²˜ë¦¬ì— íŠ¹í™”ëœ ê²½ëŸ‰ ëª¨ë¸
    """
    
    def __init__(self, 
                 input_channels: int = 3,
                 dropout_rate: float = 0.7,
                 num_classes: int = 1,
                 base_channels: int = 32):
        """
        MelSpecCNNWatermelon ëª¨ë¸ ì´ˆê¸°í™”
        
        Args:
            input_channels (int): ì…ë ¥ ì±„ë„ ìˆ˜
            dropout_rate (float): ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨
            num_classes (int): ì¶œë ¥ í´ë˜ìŠ¤ ìˆ˜ (íšŒê·€ì˜ ê²½ìš° 1)
            base_channels (int): ê¸°ë³¸ ì±„ë„ ìˆ˜
        """
        super().__init__()
        
        self.input_channels = input_channels
        self.dropout_rate = dropout_rate
        self.num_classes = num_classes
        self.base_channels = base_channels
        
        # íŠ¹ì„± ì¶”ì¶œ ë ˆì´ì–´ë“¤
        self.features = self._build_feature_layers()
        
        # ë¶„ë¥˜ê¸° ë ˆì´ì–´ë“¤
        self.classifier = self._build_classifier()
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self._initialize_weights()
        
    def _build_feature_layers(self):
        """íŠ¹ì„± ì¶”ì¶œ ë ˆì´ì–´ êµ¬ì„±"""
        layers = []
        
        # ì²« ë²ˆì§¸ ë¸”ë¡
        layers.extend([
            nn.Conv2d(self.input_channels, self.base_channels, 
                     kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(self.base_channels),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        ])
        
        # ë‘ ë²ˆì§¸ ë¸”ë¡
        layers.extend([
            nn.Conv2d(self.base_channels, self.base_channels * 2, 
                     kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(self.base_channels * 2),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        ])
        
        # ì„¸ ë²ˆì§¸ ë¸”ë¡
        layers.extend([
            nn.Conv2d(self.base_channels * 2, self.base_channels * 4, 
                     kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(self.base_channels * 4),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        ])
        
        # ë„¤ ë²ˆì§¸ ë¸”ë¡
        layers.extend([
            nn.Conv2d(self.base_channels * 4, self.base_channels * 8, 
                     kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(self.base_channels * 8),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        ])
        
        # ë‹¤ì„¯ ë²ˆì§¸ ë¸”ë¡ (ë” ê¹Šì€ íŠ¹ì„± ì¶”ì¶œ)
        layers.extend([
            nn.Conv2d(self.base_channels * 8, self.base_channels * 16, 
                     kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(self.base_channels * 16),
            nn.ReLU(inplace=True),
            nn.AdaptiveAvgPool2d((1, 1))  # Global Average Pooling
        ])
        
        return nn.Sequential(*layers)
    
    def _build_classifier(self):
        """ë¶„ë¥˜ê¸° ë ˆì´ì–´ êµ¬ì„±"""
        return nn.Sequential(
            nn.Dropout(p=self.dropout_rate),
            nn.Linear(self.base_channels * 16, self.base_channels * 4),
            nn.ReLU(inplace=True),
            nn.Dropout(p=self.dropout_rate),
            nn.Linear(self.base_channels * 4, self.base_channels),
            nn.ReLU(inplace=True),
            nn.Dropout(p=self.dropout_rate),
            nn.Linear(self.base_channels, self.num_classes)
        )
    
    def _initialize_weights(self):
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        ìˆœì „íŒŒ
        
        Args:
            x (torch.Tensor): ì…ë ¥ í…ì„œ [batch_size, channels, height, width]
            
        Returns:
            torch.Tensor: ë‹¹ë„ ì˜ˆì¸¡ê°’ [batch_size, 1]
        """
        # ì…ë ¥ í¬ê¸° ê²€ì¦
        if x.dim() != 4:
            raise ValueError(f"ì…ë ¥ì€ 4ì°¨ì› í…ì„œì—¬ì•¼ í•©ë‹ˆë‹¤. í˜„ì¬: {x.dim()}ì°¨ì›")
        
        if x.size(1) != self.input_channels:
            raise ValueError(f"ì…ë ¥ ì±„ë„ ìˆ˜ê°€ ë§ì§€ ì•ŠìŠµë‹ˆë‹¤. ì˜ˆìƒ: {self.input_channels}, í˜„ì¬: {x.size(1)}")
        
        # íŠ¹ì„± ì¶”ì¶œ
        features = self.features(x)
        
        # í‰íƒ„í™”
        features = features.view(features.size(0), -1)
        
        # ë¶„ë¥˜
        output = self.classifier(features)
        
        return output
    
    def get_feature_maps(self, x: torch.Tensor, layer_idx: Optional[int] = None) -> torch.Tensor:
        """
        ì¤‘ê°„ íŠ¹ì„± ë§µ ì¶”ì¶œ (ì‹œê°í™”ìš©)
        
        Args:
            x (torch.Tensor): ì…ë ¥ í…ì„œ
            layer_idx (int, optional): ì¶”ì¶œí•  ë ˆì´ì–´ ì¸ë±ìŠ¤
            
        Returns:
            torch.Tensor: íŠ¹ì„± ë§µ
        """
        if layer_idx is None:
            layer_idx = len(self.features) - 1
        
        with torch.no_grad():
            for i, layer in enumerate(self.features):
                x = layer(x)
                if i == layer_idx:
                    return x
        
        return x
    
    def get_model_info(self) -> Dict[str, Any]:
        """ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
        total_params = sum(p.numel() for p in self.parameters())
        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
        
        return {
            'model_name': 'MelSpecCNNWatermelon',
            'backbone': 'Custom Mel-Spectrogram CNN',
            'input_channels': self.input_channels,
            'base_channels': self.base_channels,
            'total_parameters': total_params,
            'trainable_parameters': trainable_params,
            'dropout_rate': self.dropout_rate,
            'num_classes': self.num_classes
        }
    
    def print_model_info(self):
        """ëª¨ë¸ ì •ë³´ ì¶œë ¥"""
        info = self.get_model_info()
        print(f"ğŸ¤– {info['model_name']} ëª¨ë¸ ì •ë³´")
        print(f"   ğŸ“Š ë°±ë³¸: {info['backbone']}")
        print(f"   ğŸ”§ ì…ë ¥ ì±„ë„: {info['input_channels']}")
        print(f"   ğŸ¯ ê¸°ë³¸ ì±„ë„: {info['base_channels']}")
        print(f"   ğŸ“ˆ ì´ íŒŒë¼ë¯¸í„°: {info['total_parameters']:,}")
        print(f"   ğŸ”“ í›ˆë ¨ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {info['trainable_parameters']:,}")
        print(f"   ğŸ’§ ë“œë¡­ì•„ì›ƒ: {info['dropout_rate']}")
        print(f"   ğŸ¯ ì¶œë ¥ í´ë˜ìŠ¤: {info['num_classes']}")


def create_melspec_cnn_watermelon(config_path: Optional[str] = None, **kwargs) -> MelSpecCNNWatermelon:
    """
    ì„¤ì • íŒŒì¼ ë˜ëŠ” í‚¤ì›Œë“œ ì¸ìë¡œë¶€í„° MelSpecCNNWatermelon ëª¨ë¸ ìƒì„±
    
    Args:
        config_path (str, optional): ëª¨ë¸ ì„¤ì • íŒŒì¼ ê²½ë¡œ
        **kwargs: ëª¨ë¸ íŒŒë¼ë¯¸í„° (config_pathë³´ë‹¤ ìš°ì„ )
        
    Returns:
        MelSpecCNNWatermelon: ìƒì„±ëœ ëª¨ë¸
    """
    # ê¸°ë³¸ ì„¤ì •
    default_config = {
        'input_channels': 3,
        'dropout_rate': 0.7,
        'num_classes': 1,
        'base_channels': 32
    }
    
    # ì„¤ì • íŒŒì¼ì—ì„œ ë¡œë“œ
    if config_path:
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                file_config = yaml.safe_load(f)
                default_config.update(file_config.get('model', {}))
        except FileNotFoundError:
            print(f"âš ï¸ ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_path}")
    
    # í‚¤ì›Œë“œ ì¸ìë¡œ ë®ì–´ì“°ê¸°
    default_config.update(kwargs)
    
    return MelSpecCNNWatermelon(**default_config)


def load_melspec_cnn_checkpoint(checkpoint_path: str, 
                                model_config: Optional[Dict[str, Any]] = None) -> MelSpecCNNWatermelon:
    """
    ì²´í¬í¬ì¸íŠ¸ì—ì„œ MelSpecCNN ëª¨ë¸ ë¡œë“œ
    
    Args:
        checkpoint_path (str): ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ
        model_config (dict, optional): ëª¨ë¸ ì„¤ì • (ì—†ìœ¼ë©´ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë¡œë“œ)
        
    Returns:
        MelSpecCNNWatermelon: ë¡œë“œëœ ëª¨ë¸
    """
    if not Path(checkpoint_path).exists():
        raise FileNotFoundError(f"ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {checkpoint_path}")
    
    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
    
    # ëª¨ë¸ ì„¤ì • í™•ì¸
    if model_config is None:
        model_config = checkpoint.get('model_config', {})
    
    # ëª¨ë¸ ìƒì„±
    model = MelSpecCNNWatermelon(**model_config)
    
    # ê°€ì¤‘ì¹˜ ë¡œë“œ
    model.load_state_dict(checkpoint['model_state_dict'])
    
    return model


def save_melspec_cnn_checkpoint(model: MelSpecCNNWatermelon, 
                                save_path: str, 
                                epoch: Optional[int] = None,
                                loss: Optional[float] = None,
                                optimizer_state: Optional[Dict] = None,
                                **extra_info):
    """
    MelSpecCNN ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    
    Args:
        model (MelSpecCNNWatermelon): ì €ì¥í•  ëª¨ë¸
        save_path (str): ì €ì¥ ê²½ë¡œ
        epoch (int, optional): ì—í¬í¬ ë²ˆí˜¸
        loss (float, optional): ì†ì‹¤ ê°’
        optimizer_state (dict, optional): ì˜µí‹°ë§ˆì´ì € ìƒíƒœ
        **extra_info: ì¶”ê°€ ì •ë³´
    """
    save_dir = Path(save_path).parent
    save_dir.mkdir(parents=True, exist_ok=True)
    
    # ì²´í¬í¬ì¸íŠ¸ ë°ì´í„° êµ¬ì„±
    checkpoint = {
        'model_state_dict': model.state_dict(),
        'model_config': {
            'input_channels': model.input_channels,
            'dropout_rate': model.dropout_rate,
            'num_classes': model.num_classes,
            'base_channels': model.base_channels
        },
        'model_info': model.get_model_info()
    }
    
    # ì„ íƒì  ì •ë³´ ì¶”ê°€
    if epoch is not None:
        checkpoint['epoch'] = epoch
    if loss is not None:
        checkpoint['loss'] = loss
    if optimizer_state is not None:
        checkpoint['optimizer_state_dict'] = optimizer_state
    
    # ì¶”ê°€ ì •ë³´ ë³‘í•©
    checkpoint.update(extra_info)
    
    # ì €ì¥
    torch.save(checkpoint, save_path)
    print(f"âœ… MelSpecCNN ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì™„ë£Œ: {save_path}")


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ëª¨ë¸ ìƒì„± í…ŒìŠ¤íŠ¸
    model = create_melspec_cnn_watermelon()
    model.print_model_info()
    
    # í…ŒìŠ¤íŠ¸ ì…ë ¥
    test_input = torch.randn(1, 3, 224, 224)
    output = model(test_input)
    print(f"ì¶œë ¥ í¬ê¸°: {output.shape}")
    print(f"ì¶œë ¥ ê°’: {output.item():.4f}")
    
    # íŠ¹ì„± ë§µ ì¶”ì¶œ í…ŒìŠ¤íŠ¸
    feature_maps = model.get_feature_maps(test_input, layer_idx=5)
    print(f"íŠ¹ì„± ë§µ í¬ê¸°: {feature_maps.shape}") 