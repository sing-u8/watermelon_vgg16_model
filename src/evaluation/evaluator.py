"""
Model Evaluator
ìˆ˜ë°• ë‹¹ë„ ì˜ˆì¸¡ ëª¨ë¸ì˜ ì¢…í•©ì ì¸ í‰ê°€ë¥¼ ìœ„í•œ ëª¨ë“ˆ
"""

import torch
import torch.nn as nn
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any
from pathlib import Path
import json
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report
import time
from tqdm import tqdm

# ìƒëŒ€ import
import sys
from pathlib import Path
current_dir = Path(__file__).parent
src_dir = current_dir.parent
sys.path.insert(0, str(src_dir))

from models.vgg_watermelon import VGGWatermelon, load_model_checkpoint
from training.data_loader import WatermelonDataLoader
from training.metrics import RegressionMetrics, MetricsTracker
from data.watermelon_dataset import WatermelonDataset


class WatermelonEvaluator:
    """
    ìˆ˜ë°• ë‹¹ë„ ì˜ˆì¸¡ ëª¨ë¸ì˜ ì¢…í•©ì ì¸ í‰ê°€ë¥¼ ìœ„í•œ í´ë˜ìŠ¤
    """
    
    def __init__(self, 
                 model: VGGWatermelon,
                 device: torch.device,
                 save_dir: str = "evaluation_results"):
        """
        WatermelonEvaluator ì´ˆê¸°í™”
        
        Args:
            model (VGGWatermelon): í‰ê°€í•  ëª¨ë¸
            device (torch.device): ë””ë°”ì´ìŠ¤
            save_dir (str): ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        """
        self.model = model.to(device)
        self.device = device
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ” WatermelonEvaluator ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"   ğŸ’¾ ê²°ê³¼ ì €ì¥ ê²½ë¡œ: {self.save_dir}")
        print(f"   ğŸ–¥ï¸ ë””ë°”ì´ìŠ¤: {self.device}")
    
    def evaluate_model(self, 
                      data_loader: WatermelonDataLoader,
                      split: str = "test") -> Dict[str, Any]:
        """
        ëª¨ë¸ í‰ê°€ ìˆ˜í–‰
        
        Args:
            data_loader (WatermelonDataLoader): ë°ì´í„° ë¡œë”
            split (str): í‰ê°€í•  ë°ì´í„° ë¶„í•  ('train', 'val', 'test')
            
        Returns:
            Dict[str, Any]: í‰ê°€ ê²°ê³¼
        """
        self.model.eval()
        
        # ë°ì´í„° ë¡œë” ì„ íƒ
        if split == "train":
            loader = data_loader.train_loader
        elif split == "val":
            loader = data_loader.val_loader
        elif split == "test":
            loader = data_loader.test_loader
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” split: {split}")
        
        print(f"ğŸ” {split.upper()} ë°ì´í„° í‰ê°€ ì‹œì‘")
        
        # ë©”íŠ¸ë¦­ ì´ˆê¸°í™”
        metrics = RegressionMetrics(self.device)
        
        # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
        all_predictions = []
        all_targets = []
        all_losses = []
        
        # ì¶”ë¡  ì‹œê°„ ì¸¡ì •
        inference_times = []
        
        with torch.no_grad():
            for batch_idx, (inputs, targets) in enumerate(tqdm(loader, desc=f"Evaluating {split}")):
                # ë°ì´í„°ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                inputs = inputs.to(self.device)
                targets = targets.to(self.device)
                
                # ì¶”ë¡  ì‹œê°„ ì¸¡ì •
                start_time = time.time()
                
                # ìˆœì „íŒŒ
                predictions = self.model(inputs)
                
                inference_time = time.time() - start_time
                inference_times.append(inference_time)
                
                # ì†ì‹¤ ê³„ì‚° (MSE ì‚¬ìš©)
                loss = nn.MSELoss()(predictions, targets)
                
                # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                metrics.update(predictions, targets, loss)
                
                # ê²°ê³¼ ì €ì¥
                pred_np = predictions.detach().cpu().numpy().flatten()
                target_np = targets.detach().cpu().numpy().flatten()
                
                all_predictions.extend(pred_np)
                all_targets.extend(target_np)
                all_losses.append(loss.item())
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        computed_metrics = metrics.compute()
        
        # ì¶”ê°€ ë¶„ì„
        all_predictions = np.array(all_predictions)
        all_targets = np.array(all_targets)
        
        # ì„±ëŠ¥ ë¶„ì„
        performance_analysis = self._analyze_performance(all_predictions, all_targets)
        
        # ì˜¤ì°¨ ë¶„ì„
        error_analysis = self._analyze_errors(all_predictions, all_targets)
        
        # ì¶”ë¡  ì‹œê°„ ë¶„ì„
        inference_analysis = {
            'avg_inference_time': np.mean(inference_times),
            'min_inference_time': np.min(inference_times),
            'max_inference_time': np.max(inference_times),
            'std_inference_time': np.std(inference_times),
            'total_inference_time': np.sum(inference_times),
            'samples_per_second': len(all_predictions) / np.sum(inference_times)
        }
        
        # ì¢…í•© ê²°ê³¼
        evaluation_results = {
            'split': split,
            'metrics': computed_metrics,
            'performance_analysis': performance_analysis,
            'error_analysis': error_analysis,
            'inference_analysis': inference_analysis,
            'predictions': all_predictions.tolist(),
            'targets': all_targets.tolist(),
            'losses': all_losses
        }
        
        # ê²°ê³¼ ì¶œë ¥
        self._print_evaluation_results(evaluation_results)
        
        return evaluation_results
    
    def _analyze_performance(self, predictions: np.ndarray, targets: np.ndarray) -> Dict[str, Any]:
        """ì„±ëŠ¥ ìƒì„¸ ë¶„ì„"""
        
        # ë‹¹ë„ ë²”ìœ„ë³„ ì„±ëŠ¥ ë¶„ì„
        ranges = {
            'low_sweetness': (8.0, 10.0),    # ë‚®ì€ ë‹¹ë„
            'medium_sweetness': (10.0, 11.5), # ì¤‘ê°„ ë‹¹ë„
            'high_sweetness': (11.5, 13.0)    # ë†’ì€ ë‹¹ë„
        }
        
        range_analysis = {}
        for range_name, (min_val, max_val) in ranges.items():
            mask = (targets >= min_val) & (targets < max_val)
            if np.sum(mask) > 0:
                range_preds = predictions[mask]
                range_targets = targets[mask]
                
                range_mae = np.mean(np.abs(range_preds - range_targets))
                range_rmse = np.sqrt(np.mean((range_preds - range_targets) ** 2))
                range_r2 = 1 - np.sum((range_targets - range_preds) ** 2) / np.sum((range_targets - np.mean(range_targets)) ** 2)
                
                range_analysis[range_name] = {
                    'count': int(np.sum(mask)),
                    'mae': float(range_mae),
                    'rmse': float(range_rmse),
                    'r2_score': float(range_r2),
                    'range': (min_val, max_val)
                }
        
        # ì •í™•ë„ ì„ê³„ê°’ë³„ ë¶„ì„
        accuracy_thresholds = [0.1, 0.25, 0.5, 0.75, 1.0]
        accuracy_analysis = {}
        
        errors = np.abs(predictions - targets)
        for threshold in accuracy_thresholds:
            accuracy = np.mean(errors <= threshold) * 100
            accuracy_analysis[f'accuracy_{threshold}'] = float(accuracy)
        
        return {
            'range_analysis': range_analysis,
            'accuracy_analysis': accuracy_analysis
        }
    
    def _analyze_errors(self, predictions: np.ndarray, targets: np.ndarray) -> Dict[str, Any]:
        """ì˜¤ì°¨ íŒ¨í„´ ë¶„ì„"""
        
        errors = predictions - targets
        abs_errors = np.abs(errors)
        
        # ê¸°ë³¸ ì˜¤ì°¨ í†µê³„
        error_stats = {
            'mean_error': float(np.mean(errors)),
            'std_error': float(np.std(errors)),
            'median_error': float(np.median(errors)),
            'min_error': float(np.min(errors)),
            'max_error': float(np.max(errors)),
            'mean_abs_error': float(np.mean(abs_errors)),
            'median_abs_error': float(np.median(abs_errors))
        }
        
        # ì˜¤ì°¨ ë¶„í¬ ë¶„ì„
        error_percentiles = [5, 10, 25, 75, 90, 95]
        percentile_analysis = {}
        for p in error_percentiles:
            percentile_analysis[f'percentile_{p}'] = float(np.percentile(abs_errors, p))
        
        # ì´ìƒì¹˜ ë¶„ì„ (Q3 + 1.5*IQR ì´ìƒ)
        q1 = np.percentile(abs_errors, 25)
        q3 = np.percentile(abs_errors, 75)
        iqr = q3 - q1
        outlier_threshold = q3 + 1.5 * iqr
        
        outliers = abs_errors > outlier_threshold
        outlier_analysis = {
            'outlier_threshold': float(outlier_threshold),
            'num_outliers': int(np.sum(outliers)),
            'outlier_percentage': float(np.mean(outliers) * 100),
            'outlier_indices': np.where(outliers)[0].tolist()
        }
        
        # ê³¼ì í•©/ê³¼ì†Œì í•© ë¶„ì„
        over_predictions = errors > 0  # ê³¼ëŒ€ ì˜ˆì¸¡
        under_predictions = errors < 0
        
        bias_analysis = {
            'over_prediction_rate': float(np.mean(over_predictions) * 100),
            'under_prediction_rate': float(np.mean(under_predictions) * 100),
            'avg_over_prediction': float(np.mean(errors[over_predictions])) if np.sum(over_predictions) > 0 else 0.0,
            'avg_under_prediction': float(np.mean(errors[under_predictions])) if np.sum(under_predictions) > 0 else 0.0
        }
        
        return {
            'error_stats': error_stats,
            'percentile_analysis': percentile_analysis,
            'outlier_analysis': outlier_analysis,
            'bias_analysis': bias_analysis
        }
    
    def _print_evaluation_results(self, results: Dict[str, Any]):
        """í‰ê°€ ê²°ê³¼ ì¶œë ¥"""
        split = results['split']
        metrics = results['metrics']
        perf = results['performance_analysis']
        error = results['error_analysis']
        inference = results['inference_analysis']
        
        print(f"\nğŸ“Š {split.upper()} ë°ì´í„° í‰ê°€ ê²°ê³¼")
        print("=" * 50)
        
        # ê¸°ë³¸ ë©”íŠ¸ë¦­
        print(f"ğŸ“ MAE: {metrics['mae']:.3f}")
        print(f"ğŸ“ RMSE: {metrics['rmse']:.3f}")
        print(f"ğŸ¯ RÂ² Score: {metrics['r2_score']:.3f}")
        print(f"ğŸ“ˆ MAPE: {metrics['mape']:.2f}%")
        print(f"ğŸ”— Pearson ìƒê´€ê³„ìˆ˜: {metrics['pearson_correlation']:.3f}")
        
        # ë‹¹ë„ ì •í™•ë„
        print(f"âœ… ë‹¹ë„ ì •í™•ë„ (Â±0.5): {metrics['sweetness_accuracy_05']:.1f}%")
        print(f"âœ… ë‹¹ë„ ì •í™•ë„ (Â±1.0): {metrics['sweetness_accuracy_10']:.1f}%")
        
        # ë‹¹ë„ ë²”ìœ„ë³„ ì„±ëŠ¥
        print(f"\nğŸ“Š ë‹¹ë„ ë²”ìœ„ë³„ ì„±ëŠ¥:")
        for range_name, range_data in perf['range_analysis'].items():
            print(f"   {range_name}: MAE={range_data['mae']:.3f}, "
                  f"RÂ²={range_data['r2_score']:.3f} (n={range_data['count']})")
        
        # ì˜¤ì°¨ ë¶„ì„
        print(f"\nğŸ“‰ ì˜¤ì°¨ ë¶„ì„:")
        print(f"   í‰ê·  ì˜¤ì°¨: {error['error_stats']['mean_error']:.3f} Â± {error['error_stats']['std_error']:.3f}")
        print(f"   ì¤‘ì•™ê°’ ì˜¤ì°¨: {error['error_stats']['median_error']:.3f}")
        print(f"   ì´ìƒì¹˜: {error['outlier_analysis']['num_outliers']}ê°œ ({error['outlier_analysis']['outlier_percentage']:.1f}%)")
        
        # í¸í–¥ ë¶„ì„
        print(f"   ê³¼ëŒ€ ì˜ˆì¸¡: {error['bias_analysis']['over_prediction_rate']:.1f}%")
        print(f"   ê³¼ì†Œ ì˜ˆì¸¡: {error['bias_analysis']['under_prediction_rate']:.1f}%")
        
        # ì¶”ë¡  ì„±ëŠ¥
        print(f"\nâš¡ ì¶”ë¡  ì„±ëŠ¥:")
        print(f"   í‰ê·  ì¶”ë¡  ì‹œê°„: {inference['avg_inference_time']*1000:.2f}ms")
        print(f"   ì´ˆë‹¹ ìƒ˜í”Œ ìˆ˜: {inference['samples_per_second']:.1f}")
    
    def compare_models(self, 
                      model_paths: List[str],
                      model_names: List[str],
                      data_loader: WatermelonDataLoader,
                      split: str = "test") -> Dict[str, Any]:
        """
        ì—¬ëŸ¬ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
        
        Args:
            model_paths (List[str]): ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œë“¤
            model_names (List[str]): ëª¨ë¸ ì´ë¦„ë“¤
            data_loader (WatermelonDataLoader): ë°ì´í„° ë¡œë”
            split (str): í‰ê°€í•  ë°ì´í„° ë¶„í• 
            
        Returns:
            Dict[str, Any]: ëª¨ë¸ ë¹„êµ ê²°ê³¼
        """
        print(f"ğŸ” ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ì‹œì‘ ({len(model_paths)}ê°œ ëª¨ë¸)")
        
        comparison_results = {}
        
        for model_path, model_name in zip(model_paths, model_names):
            print(f"\nğŸ“Š {model_name} í‰ê°€ ì¤‘...")
            
            # ëª¨ë¸ ë¡œë“œ
            model = load_model_checkpoint(model_path)
            model.to(self.device)
            
            # í‰ê°€ ìˆ˜í–‰
            evaluator = WatermelonEvaluator(model, self.device, 
                                          str(self.save_dir / f"comparison_{model_name}"))
            results = evaluator.evaluate_model(data_loader, split)
            
            comparison_results[model_name] = results
        
        # ë¹„êµ ë¶„ì„
        comparison_analysis = self._analyze_model_comparison(comparison_results)
        
        # ë¹„êµ ê²°ê³¼ ì €ì¥
        self._save_comparison_results(comparison_results, comparison_analysis)
        
        return {
            'individual_results': comparison_results,
            'comparison_analysis': comparison_analysis
        }
    
    def _analyze_model_comparison(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """ëª¨ë¸ ë¹„êµ ë¶„ì„"""
        
        # ë©”íŠ¸ë¦­ë³„ ìˆœìœ„
        metrics_comparison = {}
        metric_names = ['mae', 'rmse', 'r2_score', 'mape', 'sweetness_accuracy_05', 'sweetness_accuracy_10']
        
        for metric in metric_names:
            metric_values = {}
            for model_name, result in results.items():
                metric_values[model_name] = result['metrics'][metric]
            
            # ì •ë ¬ (MAE, RMSE, MAPEëŠ” ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ, ë‚˜ë¨¸ì§€ëŠ” ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
            reverse = metric not in ['mae', 'rmse', 'mape']
            sorted_models = sorted(metric_values.items(), key=lambda x: x[1], reverse=reverse)
            
            metrics_comparison[metric] = {
                'ranking': [(name, value) for name, value in sorted_models],
                'best_model': sorted_models[0][0],
                'best_value': sorted_models[0][1]
            }
        
        # ì¢…í•© ìˆœìœ„ (ì—¬ëŸ¬ ë©”íŠ¸ë¦­ì˜ ìˆœìœ„ í‰ê· )
        model_ranks = {name: [] for name in results.keys()}
        
        for metric, comparison in metrics_comparison.items():
            for rank, (model_name, _) in enumerate(comparison['ranking']):
                model_ranks[model_name].append(rank + 1)
        
        overall_ranking = []
        for model_name, ranks in model_ranks.items():
            avg_rank = np.mean(ranks)
            overall_ranking.append((model_name, avg_rank))
        
        overall_ranking.sort(key=lambda x: x[1])
        
        return {
            'metrics_comparison': metrics_comparison,
            'overall_ranking': overall_ranking,
            'best_overall_model': overall_ranking[0][0]
        }
    
    def _save_comparison_results(self, results: Dict[str, Any], analysis: Dict[str, Any]):
        """ëª¨ë¸ ë¹„êµ ê²°ê³¼ ì €ì¥"""
        
        # JSON ì €ì¥
        comparison_data = {
            'comparison_results': results,
            'comparison_analysis': analysis
        }
        
        save_path = self.save_dir / "model_comparison.json"
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(comparison_data, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"ğŸ’¾ ëª¨ë¸ ë¹„êµ ê²°ê³¼ ì €ì¥ë¨: {save_path}")
    
    def generate_evaluation_report(self, 
                                  evaluation_results: Dict[str, Any],
                                  include_plots: bool = True) -> str:
        """
        ì¢…í•© í‰ê°€ ë³´ê³ ì„œ ìƒì„±
        
        Args:
            evaluation_results (Dict): í‰ê°€ ê²°ê³¼
            include_plots (bool): í”Œë¡¯ í¬í•¨ ì—¬ë¶€
            
        Returns:
            str: ìƒì„±ëœ ë³´ê³ ì„œ ê²½ë¡œ
        """
        print("ğŸ“ í‰ê°€ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        
        # ë³´ê³ ì„œ ë‚´ìš© ìƒì„±
        report_content = self._generate_report_content(evaluation_results)
        
        # ë³´ê³ ì„œ ì €ì¥
        report_path = self.save_dir / "evaluation_report.md"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        # í”Œë¡¯ ìƒì„±
        if include_plots:
            self._generate_evaluation_plots(evaluation_results)
        
        print(f"ğŸ“„ í‰ê°€ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: {report_path}")
        return str(report_path)
    
    def _generate_report_content(self, results: Dict[str, Any]) -> str:
        """ë³´ê³ ì„œ ë‚´ìš© ìƒì„±"""
        
        split = results['split']
        metrics = results['metrics']
        perf = results['performance_analysis']
        error = results['error_analysis']
        inference = results['inference_analysis']
        
        report = f"""# ğŸ‰ Watermelon Sweetness Prediction - Evaluation Report

## ğŸ“Š Overview
- **Dataset Split**: {split.upper()}
- **Total Samples**: {metrics['num_samples']:,}
- **Evaluation Date**: {time.strftime('%Y-%m-%d %H:%M:%S')}

## ğŸ¯ Model Performance

### Core Metrics
| Metric | Value |
|--------|-------|
| MAE (Mean Absolute Error) | {metrics['mae']:.3f} |
| RMSE (Root Mean Square Error) | {metrics['rmse']:.3f} |
| RÂ² Score | {metrics['r2_score']:.3f} |
| MAPE (Mean Absolute Percentage Error) | {metrics['mape']:.2f}% |
| Pearson Correlation | {metrics['pearson_correlation']:.3f} |

### Sweetness Accuracy
| Tolerance | Accuracy |
|-----------|----------|
| Â±0.5 Brix | {metrics['sweetness_accuracy_05']:.1f}% |
| Â±1.0 Brix | {metrics['sweetness_accuracy_10']:.1f}% |

## ğŸ“ˆ Performance by Sweetness Range

"""
        
        # ë‹¹ë„ ë²”ìœ„ë³„ ì„±ëŠ¥ ì¶”ê°€
        for range_name, range_data in perf['range_analysis'].items():
            range_display = range_name.replace('_', ' ').title()
            report += f"### {range_display} ({range_data['range'][0]}-{range_data['range'][1]} Brix)\n"
            report += f"- **Samples**: {range_data['count']}\n"
            report += f"- **MAE**: {range_data['mae']:.3f}\n"
            report += f"- **RMSE**: {range_data['rmse']:.3f}\n"
            report += f"- **RÂ² Score**: {range_data['r2_score']:.3f}\n\n"
        
        # ì˜¤ì°¨ ë¶„ì„ ì¶”ê°€
        report += f"""## ğŸ“‰ Error Analysis

### Error Statistics
- **Mean Error**: {error['error_stats']['mean_error']:.3f} Â± {error['error_stats']['std_error']:.3f}
- **Median Error**: {error['error_stats']['median_error']:.3f}
- **Max Absolute Error**: {error['error_stats']['max_error']:.3f}

### Prediction Bias
- **Over-prediction Rate**: {error['bias_analysis']['over_prediction_rate']:.1f}%
- **Under-prediction Rate**: {error['bias_analysis']['under_prediction_rate']:.1f}%

### Outliers
- **Outlier Threshold**: {error['outlier_analysis']['outlier_threshold']:.3f}
- **Number of Outliers**: {error['outlier_analysis']['num_outliers']} ({error['outlier_analysis']['outlier_percentage']:.1f}%)

## âš¡ Inference Performance

| Metric | Value |
|--------|-------|
| Average Inference Time | {inference['avg_inference_time']*1000:.2f} ms |
| Samples per Second | {inference['samples_per_second']:.1f} |
| Total Inference Time | {inference['total_inference_time']:.2f} s |

## ğŸ¯ Model Assessment

### Strengths
"""
        
        # ê°•ì  ë¶„ì„
        strengths = []
        if metrics['r2_score'] > 0.8:
            strengths.append("- Excellent correlation between predictions and actual values (RÂ² > 0.8)")
        if metrics['sweetness_accuracy_05'] > 70:
            strengths.append("- High accuracy within Â±0.5 Brix tolerance")
        if inference['avg_inference_time'] < 0.1:
            strengths.append("- Fast inference suitable for real-time applications")
        
        for strength in strengths:
            report += f"{strength}\n"
        
        # ê°œì„  ì˜ì—­
        report += "\n### Areas for Improvement\n"
        improvements = []
        if metrics['mae'] > 0.5:
            improvements.append("- Consider reducing MAE for better accuracy")
        if error['bias_analysis']['over_prediction_rate'] > 60 or error['bias_analysis']['under_prediction_rate'] > 60:
            improvements.append("- Address prediction bias for more balanced predictions")
        if error['outlier_analysis']['outlier_percentage'] > 10:
            improvements.append("- Investigate and reduce outlier predictions")
        
        for improvement in improvements:
            report += f"{improvement}\n"
        
        report += f"""
## ğŸ“Š Visualizations

The following plots are available in the evaluation results:
- Prediction vs Target Scatter Plot
- Error Distribution Histogram
- Residual Plot
- Performance by Sweetness Range

---
*Report generated by WatermelonEvaluator*
"""
        
        return report
    
    def _generate_evaluation_plots(self, results: Dict[str, Any]):
        """í‰ê°€ í”Œë¡¯ ìƒì„±"""
        
        predictions = np.array(results['predictions'])
        targets = np.array(results['targets'])
        
        # í”Œë¡¯ ìŠ¤íƒ€ì¼ ì„¤ì •
        plt.style.use('default')
        sns.set_palette("husl")
        
        # 1. Prediction vs Target ì‚°ì ë„
        plt.figure(figsize=(10, 8))
        plt.scatter(targets, predictions, alpha=0.6, s=50, edgecolors='black', linewidth=0.5)
        
        # ì™„ë²½í•œ ì˜ˆì¸¡ ë¼ì¸
        min_val = min(targets.min(), predictions.min())
        max_val = max(targets.max(), predictions.max())
        plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')
        
        # í—ˆìš© ì˜¤ì°¨ ì˜ì—­
        plt.fill_between([min_val, max_val], [min_val-0.5, max_val-0.5], 
                        [min_val+0.5, max_val+0.5], alpha=0.2, color='green', 
                        label='Â±0.5 Brix tolerance')
        
        plt.xlabel('Actual Sweetness (Brix)', fontsize=12)
        plt.ylabel('Predicted Sweetness (Brix)', fontsize=12)
        plt.title('ğŸ‰ Watermelon Sweetness Prediction Results', fontsize=14)
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.save_dir / "prediction_scatter.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. ì˜¤ì°¨ ë¶„í¬ íˆìŠ¤í† ê·¸ë¨
        errors = predictions - targets
        
        plt.figure(figsize=(12, 8))
        
        plt.subplot(2, 2, 1)
        plt.hist(errors, bins=30, alpha=0.7, color='skyblue', edgecolor='black')
        plt.axvline(float(np.mean(errors)), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(errors):.3f}')
        plt.xlabel('Prediction Error (Brix)')
        plt.ylabel('Frequency')
        plt.title('Error Distribution')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 3. ì ˆëŒ€ ì˜¤ì°¨ ë¶„í¬
        plt.subplot(2, 2, 2)
        abs_errors = np.abs(errors)
        plt.hist(abs_errors, bins=30, alpha=0.7, color='lightcoral', edgecolor='black')
        plt.axvline(float(np.mean(abs_errors)), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(abs_errors):.3f}')
        plt.xlabel('Absolute Error (Brix)')
        plt.ylabel('Frequency')
        plt.title('Absolute Error Distribution')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 4. ì”ì°¨ í”Œë¡¯
        plt.subplot(2, 2, 3)
        plt.scatter(predictions, errors, alpha=0.6, s=30)
        plt.axhline(0, color='red', linestyle='--', linewidth=2)
        plt.xlabel('Predicted Sweetness (Brix)')
        plt.ylabel('Residuals (Brix)')
        plt.title('Residual Plot')
        plt.grid(True, alpha=0.3)
        
        # 5. Q-Q í”Œë¡¯
        plt.subplot(2, 2, 4)
        from scipy import stats
        stats.probplot(errors, dist="norm", plot=plt)
        plt.title('Q-Q Plot (Normal Distribution)')
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(self.save_dir / "error_analysis.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š í‰ê°€ í”Œë¡¯ ì €ì¥ë¨: {self.save_dir}")


# ì‚¬ìš© í¸ì˜ë¥¼ ìœ„í•œ í—¬í¼ í•¨ìˆ˜ë“¤
def create_evaluator(model_path: str, device: str = "auto") -> WatermelonEvaluator:
    """
    í‰ê°€ê¸°ë¥¼ ì‰½ê²Œ ìƒì„±í•˜ëŠ” í—¬í¼ í•¨ìˆ˜
    
    Args:
        model_path (str): ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
        device (str): ë””ë°”ì´ìŠ¤ ('auto', 'cpu', 'cuda')
        
    Returns:
        WatermelonEvaluator: ì´ˆê¸°í™”ëœ í‰ê°€ê¸°
    """
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    if device == "auto":
        torch_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    else:
        torch_device = torch.device(device)
    
    # ëª¨ë¸ ë¡œë“œ
    model = load_model_checkpoint(model_path)
    model.to(torch_device)
    
    # í‰ê°€ê¸° ìƒì„±
    evaluator = WatermelonEvaluator(model, torch_device)
    
    return evaluator


def quick_evaluation(model_path: str, data_path: str, split: str = "test") -> Dict[str, Any]:
    """
    ë¹ ë¥¸ ëª¨ë¸ í‰ê°€ë¥¼ ìœ„í•œ í—¬í¼ í•¨ìˆ˜
    
    Args:
        model_path (str): ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
        data_path (str): ë°ì´í„°ì…‹ ê²½ë¡œ
        split (str): í‰ê°€í•  ë¶„í• 
        
    Returns:
        Dict[str, Any]: í‰ê°€ ê²°ê³¼
    """
    # í‰ê°€ê¸° ìƒì„±
    evaluator = create_evaluator(model_path)
    
    # ë°ì´í„° ë¡œë” ìƒì„±
    from training.data_loader import create_data_loaders
    data_loader = create_data_loaders(
        data_path=data_path,
        batch_size=32,
        num_workers=4
    )
    
    # í‰ê°€ ìˆ˜í–‰
    results = evaluator.evaluate_model(data_loader, split)
    
    # ë³´ê³ ì„œ ìƒì„±
    evaluator.generate_evaluation_report(results)
    
    return results


if __name__ == "__main__":
    # í‰ê°€ê¸° í…ŒìŠ¤íŠ¸
    print("ğŸ§ª í‰ê°€ê¸° ëª¨ë“ˆ í…ŒìŠ¤íŠ¸")
    
    try:
        # ë”ë¯¸ ëª¨ë¸ ìƒì„±
        from models.vgg_watermelon import create_vgg_watermelon
        model = create_vgg_watermelon()
        
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # í‰ê°€ê¸° ìƒì„±
        evaluator = WatermelonEvaluator(model, device, "test_evaluation")
        
        print("âœ… í‰ê°€ê¸° í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ í‰ê°€ê¸° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc() 