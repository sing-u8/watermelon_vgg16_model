#!/usr/bin/env python3
"""
ëª¨ë¸ ë³€í™˜ ìŠ¤í¬ë¦½íŠ¸: PyTorch â†’ ONNX â†’ Core ML
iOS ë°°í¬ë¥¼ ìœ„í•œ ëª¨ë¸ ë³€í™˜ ë° ìµœì í™”
"""

import torch
import torch.onnx
import onnx
try:
    import onnxsim
    ONNXSIM_AVAILABLE = True
except ImportError:
    ONNXSIM_AVAILABLE = False
    print("âš ï¸ onnxsim not available, skipping model simplification")
import coremltools as ct
import numpy as np
from pathlib import Path
import argparse
import json
import time
from typing import Tuple, Dict, Any
import sys
import os

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.models.vgg_watermelon import VGGWatermelon

class ModelConverter:
    """ëª¨ë¸ ë³€í™˜ í´ë˜ìŠ¤"""
    
    def __init__(self, model_path: str, output_dir: str = "models/converted"):
        """
        ì´ˆê¸°í™”
        
        Args:
            model_path: PyTorch ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
            output_dir: ë³€í™˜ëœ ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬
        """
        self.model_path = Path(model_path)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ëª¨ë¸ ë¡œë“œ
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = self._load_model()
        
        # ë³€í™˜ ê²°ê³¼ ì €ì¥ìš©
        self.conversion_results = {}
        
    def _load_model(self) -> torch.nn.Module:
        """PyTorch ëª¨ë¸ ë¡œë“œ"""
        try:
            print(f"ğŸ”„ PyTorch ëª¨ë¸ ë¡œë”©: {self.model_path}")
            
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (PyTorch 2.6+ í˜¸í™˜ì„±)
            checkpoint = torch.load(self.model_path, map_location=self.device, weights_only=False)
            
            # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ì„¤ì • ì¶”ì¶œ
            model_config = checkpoint.get('model_config', {})
            model_name = model_config.get('model_name', 'VGGWatermelon')
            backbone = model_config.get('backbone', 'VGG-16')
            fc_hidden_size = model_config.get('fc_hidden_size', 512)
            dropout_rate = model_config.get('dropout_rate', 0.5)
            
            print(f"ğŸ“Š ëª¨ë¸ ì •ë³´: {model_name} (ë°±ë³¸: {backbone})")
            print(f"ğŸ“Š ëª¨ë¸ ì„¤ì •: FC Hidden Size={fc_hidden_size}, Dropout={dropout_rate}")
            
            # ëª¨ë¸ íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ ëª¨ë¸ ìƒì„±
            if 'EfficientNet' in model_name:
                print("ğŸ”§ EfficientNet ëª¨ë¸ ë¡œë”©...")
                from src.models.efficientnet_watermelon import create_efficientnet_watermelon
                model = create_efficientnet_watermelon(
                    pretrained=False,
                    dropout_rate=dropout_rate,
                    num_fc_layers=model_config.get('fc_layers', 2),
                    fc_hidden_size=fc_hidden_size
                )
            elif 'MelSpecCNN' in model_name:
                print("ğŸ”§ MelSpecCNN ëª¨ë¸ ë¡œë”©...")
                from src.models.melspec_cnn_watermelon import create_melspec_cnn_watermelon
                model = create_melspec_cnn_watermelon(
                    input_channels=model_config.get('input_channels', 3),
                    base_channels=model_config.get('base_channels', 32),
                    dropout_rate=dropout_rate
                )
            else:
                print("ğŸ”§ VGG ëª¨ë¸ ë¡œë”©...")
                # ê¸°ì¡´ VGG ëª¨ë¸ ë¡œë”©
                model = VGGWatermelon(
                    input_channels=3,
                    pretrained=False,  # ì´ë¯¸ í›ˆë ¨ëœ ëª¨ë¸ì´ë¯€ë¡œ False
                    fc_hidden_size=fc_hidden_size,
                    dropout_rate=dropout_rate
                )
            
            # ê°€ì¤‘ì¹˜ ë¡œë“œ
            if 'model_state_dict' in checkpoint:
                model.load_state_dict(checkpoint['model_state_dict'])
                print(f"âœ… ëª¨ë¸ ìƒíƒœ ë¡œë“œ ì™„ë£Œ (ì—í¬í¬: {checkpoint.get('epoch', 'N/A')})")
            else:
                model.load_state_dict(checkpoint)
                print(f"âœ… ëª¨ë¸ ìƒíƒœ ë¡œë“œ ì™„ë£Œ")
            
            model.eval()
            model.to(self.device)
            
            # ëª¨ë¸ ì •ë³´ ì¶œë ¥
            total_params = sum(p.numel() for p in model.parameters())
            print(f"ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}")
            
            return model
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    def create_dummy_input(self, batch_size: int = 1) -> torch.Tensor:
        """
        ë”ë¯¸ ì…ë ¥ ìƒì„± (ë©œ-ìŠ¤í™íŠ¸ë¡œê·¸ë¨ í˜•íƒœ)
        
        Args:
            batch_size: ë°°ì¹˜ í¬ê¸°
            
        Returns:
            ë”ë¯¸ ì…ë ¥ í…ì„œ (B, C, H, W)
        """
        # VGG-16 ì…ë ¥ í¬ê¸°: 224x224x3
        input_shape = (batch_size, 3, 224, 224)
        dummy_input = torch.randn(input_shape, device=self.device)
        
        print(f"ğŸ“‹ ë”ë¯¸ ì…ë ¥ í¬ê¸°: {input_shape}")
        return dummy_input
    
    def convert_to_onnx(self, 
                       output_name: str = None,
                       opset_version: int = 11,
                       simplify: bool = True,
                       batch_size: int = 1) -> str:
        """
        PyTorch â†’ ONNX ë³€í™˜
        
        Args:
            output_name: ì¶œë ¥ íŒŒì¼ëª…
            opset_version: ONNX opset ë²„ì „
            simplify: ONNX ëª¨ë¸ ìµœì í™” ì—¬ë¶€
            batch_size: ë°°ì¹˜ í¬ê¸°
            
        Returns:
            ë³€í™˜ëœ ONNX íŒŒì¼ ê²½ë¡œ
        """
        try:
            if output_name is None:
                output_name = f"{self.model_path.stem}_batch{batch_size}.onnx"
            
            onnx_path = self.output_dir / output_name
            
            print(f"ğŸ”„ ONNX ë³€í™˜ ì‹œì‘...")
            print(f"   - ì¶œë ¥ ê²½ë¡œ: {onnx_path}")
            print(f"   - Opset ë²„ì „: {opset_version}")
            print(f"   - ë°°ì¹˜ í¬ê¸°: {batch_size}")
            
            # ë”ë¯¸ ì…ë ¥ ìƒì„±
            dummy_input = self.create_dummy_input(batch_size)
            
            # ë™ì  ë°°ì¹˜ í¬ê¸° ì„¤ì •
            dynamic_axes = {
                'input': {0: 'batch_size'},
                'output': {0: 'batch_size'}
            }
            
            # ONNX ë³€í™˜
            start_time = time.time()
            
            torch.onnx.export(
                self.model,
                dummy_input,
                str(onnx_path),
                export_params=True,
                opset_version=opset_version,
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes=dynamic_axes,
                verbose=False
            )
            
            conversion_time = time.time() - start_time
            
            # ONNX ëª¨ë¸ ê²€ì¦
            onnx_model = onnx.load(str(onnx_path))
            onnx.checker.check_model(onnx_model)
            
            # ëª¨ë¸ ìµœì í™” (ONNX Simplifier)
            if simplify and ONNXSIM_AVAILABLE:
                print(f"ğŸ”§ ONNX ëª¨ë¸ ìµœì í™” ì¤‘...")
                try:
                    simplified_model, check = onnxsim.simplify(onnx_model)
                    if check:
                        onnx.save(simplified_model, str(onnx_path))
                        print(f"âœ… ONNX ëª¨ë¸ ìµœì í™” ì™„ë£Œ")
                    else:
                        print(f"âš ï¸ ONNX ëª¨ë¸ ìµœì í™” ì‹¤íŒ¨, ì›ë³¸ ì‚¬ìš©")
                except Exception as e:
                    print(f"âš ï¸ ONNX ìµœì í™” ê±´ë„ˆëœ€: {e}")
            elif simplify and not ONNXSIM_AVAILABLE:
                print(f"âš ï¸ ONNX Simplifier ë¯¸ì„¤ì¹˜, ìµœì í™” ê±´ë„ˆëœ€")
            
            # íŒŒì¼ í¬ê¸° í™•ì¸
            file_size = onnx_path.stat().st_size / (1024 * 1024)  # MB
            
            print(f"âœ… ONNX ë³€í™˜ ì™„ë£Œ!")
            print(f"   - ë³€í™˜ ì‹œê°„: {conversion_time:.2f}ì´ˆ")
            print(f"   - íŒŒì¼ í¬ê¸°: {file_size:.2f}MB")
            
            # ê²°ê³¼ ì €ì¥
            self.conversion_results['onnx'] = {
                'path': str(onnx_path),
                'size_mb': file_size,
                'conversion_time': conversion_time,
                'opset_version': opset_version,
                'batch_size': batch_size
            }
            
            return str(onnx_path)
            
        except Exception as e:
            print(f"âŒ ONNX ë³€í™˜ ì‹¤íŒ¨: {e}")
            raise
    
    def convert_to_coreml(self, 
                         onnx_path: str = None,
                         output_name: str = None,
                         compute_units: str = "ALL",
                         quantize: bool = True) -> str:
        """
        ONNX â†’ Core ML ë³€í™˜
        
        Args:
            onnx_path: ONNX ëª¨ë¸ ê²½ë¡œ
            output_name: ì¶œë ¥ íŒŒì¼ëª…
            compute_units: ì—°ì‚° ìœ ë‹› (ALL, CPU_ONLY, CPU_AND_GPU)
            quantize: ì–‘ìí™” ì ìš© ì—¬ë¶€
            
        Returns:
            ë³€í™˜ëœ Core ML íŒŒì¼ ê²½ë¡œ
        """
        try:
            if onnx_path is None:
                # ê°€ì¥ ìµœê·¼ ë³€í™˜ëœ ONNX íŒŒì¼ ì‚¬ìš©
                onnx_files = list(self.output_dir.glob("*.onnx"))
                if not onnx_files:
                    raise ValueError("ONNX íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ONNX ë³€í™˜ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
                onnx_path = max(onnx_files, key=lambda x: x.stat().st_mtime)
            
            if output_name is None:
                output_name = f"{Path(onnx_path).stem}.mlmodel"
            
            coreml_path = self.output_dir / output_name
            
            print(f"ğŸ”„ Core ML ë³€í™˜ ì‹œì‘...")
            print(f"   - ONNX ê²½ë¡œ: {onnx_path}")
            print(f"   - ì¶œë ¥ ê²½ë¡œ: {coreml_path}")
            print(f"   - ì—°ì‚° ìœ ë‹›: {compute_units}")
            print(f"   - ì–‘ìí™”: {quantize}")
            
            start_time = time.time()
            
            # Core ML ë³€í™˜
            # ì…ë ¥ í˜•íƒœ ì •ì˜
            input_shape = ct.Shape(shape=(1, 3, 224, 224))  # (batch, channels, height, width)
            
            # ë³€í™˜ ì˜µì…˜ ì„¤ì •
            convert_options = {
                'inputs': [ct.TensorType(name='input', shape=input_shape)],
                'outputs': [ct.TensorType(name='output')],
                'compute_units': getattr(ct.ComputeUnit, compute_units),
            }
            
            # ì–‘ìí™” ì„¤ì •
            if quantize:
                # 16-bit ì–‘ìí™” (iOSì—ì„œ ê¶Œì¥)
                convert_options['compute_precision'] = ct.precision.FLOAT16
            
            # ONNX ëª¨ë¸ ë¡œë“œ ë° ë³€í™˜
            coreml_model = ct.convert(
                onnx_path,
                source="onnx",
                **convert_options
            )
            
            # ëª¨ë¸ ë©”íƒ€ë°ì´í„° ì„¤ì •
            coreml_model.short_description = "ğŸ‰ Watermelon Sweetness Prediction Model"
            coreml_model.input_description['input'] = "Mel-spectrogram of watermelon tapping sound (224x224x3)"
            coreml_model.output_description['output'] = "Predicted sweetness value in Brix"
            
            # ë²„ì „ ì •ë³´
            coreml_model.version = "1.0"
            coreml_model.author = "Watermelon ML Team"
            
            # ëª¨ë¸ ì €ì¥
            coreml_model.save(str(coreml_path))
            
            conversion_time = time.time() - start_time
            
            # íŒŒì¼ í¬ê¸° í™•ì¸
            file_size = coreml_path.stat().st_size / (1024 * 1024)  # MB
            
            print(f"âœ… Core ML ë³€í™˜ ì™„ë£Œ!")
            print(f"   - ë³€í™˜ ì‹œê°„: {conversion_time:.2f}ì´ˆ")
            print(f"   - íŒŒì¼ í¬ê¸°: {file_size:.2f}MB")
            
            # ëª¨ë¸ ê²€ì¦
            self._validate_coreml_model(str(coreml_path))
            
            # ê²°ê³¼ ì €ì¥
            self.conversion_results['coreml'] = {
                'path': str(coreml_path),
                'size_mb': file_size,
                'conversion_time': conversion_time,
                'compute_units': compute_units,
                'quantized': quantize
            }
            
            return str(coreml_path)
            
        except Exception as e:
            print(f"âŒ Core ML ë³€í™˜ ì‹¤íŒ¨: {e}")
            raise
    
    def _validate_coreml_model(self, coreml_path: str):
        """Core ML ëª¨ë¸ ê²€ì¦"""
        try:
            print(f"ğŸ” Core ML ëª¨ë¸ ê²€ì¦ ì¤‘...")
            
            # Core ML ëª¨ë¸ ë¡œë“œ
            model = ct.models.MLModel(coreml_path)
            
            # ëª¨ë¸ ì •ë³´ ì¶œë ¥
            spec = model.get_spec()
            print(f"   - ëª¨ë¸ íƒ€ì…: {spec.WhichOneof('Type')}")
            print(f"   - Core ML ë²„ì „: {spec.specificationVersion}")
            
            # ì…ë ¥/ì¶œë ¥ ì •ë³´
            for input_desc in spec.description.input:
                print(f"   - ì…ë ¥: {input_desc.name} {input_desc.type}")
            
            for output_desc in spec.description.output:
                print(f"   - ì¶œë ¥: {output_desc.name} {output_desc.type}")
            
            print(f"âœ… Core ML ëª¨ë¸ ê²€ì¦ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âš ï¸ Core ML ëª¨ë¸ ê²€ì¦ ì‹¤íŒ¨: {e}")
    
    def test_inference(self, onnx_path: str = None, coreml_path: str = None) -> Dict[str, Any]:
        """
        ë³€í™˜ëœ ëª¨ë¸ë“¤ì˜ ì¶”ë¡  ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        
        Args:
            onnx_path: ONNX ëª¨ë¸ ê²½ë¡œ
            coreml_path: Core ML ëª¨ë¸ ê²½ë¡œ
            
        Returns:
            ì¶”ë¡  ì„±ëŠ¥ ê²°ê³¼
        """
        results = {}
        
        # í…ŒìŠ¤íŠ¸ ì…ë ¥ ì¤€ë¹„
        test_input = self.create_dummy_input(batch_size=1)
        test_input_np = test_input.cpu().numpy()
        
        print(f"ğŸ§ª ì¶”ë¡  ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        # 1. PyTorch ëª¨ë¸ í…ŒìŠ¤íŠ¸
        try:
            with torch.no_grad():
                start_time = time.time()
                pytorch_output = self.model(test_input)
                pytorch_time = time.time() - start_time
                
                results['pytorch'] = {
                    'inference_time': pytorch_time,
                    'output_shape': list(pytorch_output.shape),
                    'output_sample': float(pytorch_output[0].item())
                }
                
                print(f"   âœ… PyTorch: {pytorch_time*1000:.2f}ms")
                
        except Exception as e:
            print(f"   âŒ PyTorch í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        # 2. ONNX ëª¨ë¸ í…ŒìŠ¤íŠ¸
        if onnx_path and Path(onnx_path).exists():
            try:
                import onnxruntime as ort
                
                session = ort.InferenceSession(onnx_path)
                
                start_time = time.time()
                onnx_output = session.run(['output'], {'input': test_input_np})
                onnx_time = time.time() - start_time
                
                results['onnx'] = {
                    'inference_time': onnx_time,
                    'output_shape': list(onnx_output[0].shape),
                    'output_sample': float(onnx_output[0][0])
                }
                
                print(f"   âœ… ONNX: {onnx_time*1000:.2f}ms")
                
            except Exception as e:
                print(f"   âŒ ONNX í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        # 3. Core ML ëª¨ë¸ í…ŒìŠ¤íŠ¸
        if coreml_path and Path(coreml_path).exists():
            try:
                import coremltools as ct
                
                model = ct.models.MLModel(coreml_path)
                
                # ì…ë ¥ ë°ì´í„° ì¤€ë¹„ (Core ML í˜•ì‹)
                input_dict = {'input': test_input_np}
                
                start_time = time.time()
                coreml_output = model.predict(input_dict)
                coreml_time = time.time() - start_time
                
                output_value = list(coreml_output.values())[0]
                
                results['coreml'] = {
                    'inference_time': coreml_time,
                    'output_shape': list(output_value.shape) if hasattr(output_value, 'shape') else [1],
                    'output_sample': float(output_value[0]) if hasattr(output_value, '__getitem__') else float(output_value)
                }
                
                print(f"   âœ… Core ML: {coreml_time*1000:.2f}ms")
                
            except Exception as e:
                print(f"   âŒ Core ML í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        return results
    
    def save_conversion_report(self, inference_results: Dict = None):
        """ë³€í™˜ ê²°ê³¼ ë¦¬í¬íŠ¸ ì €ì¥"""
        report = {
            'model_info': {
                'source_model': str(self.model_path),
                'conversion_timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
                'device': str(self.device)
            },
            'conversion_results': self.conversion_results,
            'inference_results': inference_results or {}
        }
        
        report_path = self.output_dir / 'conversion_report.json'
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ ë³€í™˜ ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")
        
        # ìš”ì•½ ì¶œë ¥
        print(f"\n{'='*50}")
        print(f"ğŸ¯ ëª¨ë¸ ë³€í™˜ ì™„ë£Œ ìš”ì•½")
        print(f"{'='*50}")
        
        for format_name, result in self.conversion_results.items():
            print(f"ğŸ“± {format_name.upper()}:")
            print(f"   - íŒŒì¼: {result['path']}")
            print(f"   - í¬ê¸°: {result['size_mb']:.2f}MB")
            print(f"   - ë³€í™˜ ì‹œê°„: {result['conversion_time']:.2f}ì´ˆ")
        
        if inference_results:
            print(f"\nâš¡ ì¶”ë¡  ì„±ëŠ¥ ë¹„êµ:")
            for format_name, result in inference_results.items():
                time_ms = result['inference_time'] * 1000
                print(f"   - {format_name.upper()}: {time_ms:.2f}ms")

def main():
    parser = argparse.ArgumentParser(description="PyTorch ëª¨ë¸ì„ ONNXì™€ Core MLë¡œ ë³€í™˜")
    parser.add_argument("model_path", help="PyTorch ëª¨ë¸ íŒŒì¼ ê²½ë¡œ (.pth)")
    parser.add_argument("--output-dir", default="models/converted", help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--batch-size", type=int, default=1, help="ë°°ì¹˜ í¬ê¸°")
    parser.add_argument("--opset-version", type=int, default=11, help="ONNX opset ë²„ì „")
    parser.add_argument("--compute-units", default="ALL", 
                       choices=["ALL", "CPU_ONLY", "CPU_AND_GPU"],
                       help="Core ML ì—°ì‚° ìœ ë‹›")
    parser.add_argument("--no-quantize", action="store_true", help="Core ML ì–‘ìí™” ë¹„í™œì„±í™”")
    parser.add_argument("--no-simplify", action="store_true", help="ONNX ìµœì í™” ë¹„í™œì„±í™”")
    parser.add_argument("--test-inference", action="store_true", help="ì¶”ë¡  ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
    parser.add_argument("--onnx-only", action="store_true", help="ONNX ë³€í™˜ë§Œ ì‹¤í–‰")
    parser.add_argument("--coreml-only", action="store_true", help="Core ML ë³€í™˜ë§Œ ì‹¤í–‰")
    
    args = parser.parse_args()
    
    # ë³€í™˜ê¸° ì´ˆê¸°í™”
    converter = ModelConverter(args.model_path, args.output_dir)
    
    onnx_path = None
    coreml_path = None
    
    try:
        # ONNX ë³€í™˜
        if not args.coreml_only:
            onnx_path = converter.convert_to_onnx(
                batch_size=args.batch_size,
                opset_version=args.opset_version,
                simplify=not args.no_simplify
            )
        
        # Core ML ë³€í™˜
        if not args.onnx_only:
            coreml_path = converter.convert_to_coreml(
                onnx_path=onnx_path,
                compute_units=args.compute_units,
                quantize=not args.no_quantize
            )
        
        # ì¶”ë¡  ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        inference_results = None
        if args.test_inference:
            inference_results = converter.test_inference(onnx_path, coreml_path)
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        converter.save_conversion_report(inference_results)
        
    except Exception as e:
        print(f"âŒ ë³€í™˜ ì‹¤íŒ¨: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main()) 